import json

from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.responses import StreamingResponse

from app.models import (
    CaseSearchResponse,
    CombinedSearchResponse,
    QueryRequest,
    WebSearchResponse,
)
from app.security import verify_api_key, verify_api_key_query
from app.services.llm import (
    answer_based_on_cases,
    answer_based_on_cases_stream,
    get_openai_client,
    get_sonar_answer,
)
from app.services.qdrant import get_cases_from_qdrant

router = APIRouter(tags=["search"])


# Web Search (Sonar) Only Endpoints
@router.post("/web-search", response_model=WebSearchResponse)
async def web_search(
    request: QueryRequest, api_key_valid: bool = Depends(verify_api_key)
):
    """
    Web search using Perplexity Sonar only
    Returns answer with citations but no case information
    """
    try:
        answer, citations = await get_sonar_answer(request.question)

        return WebSearchResponse(
            answer=answer,
            source="Perplexity Sonar via OpenRouter",
            citations=citations,
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/web-search-stream")
async def web_search_stream(
    question: str = Query(..., description="Legal question to search"),
    api_key_valid: bool = Depends(verify_api_key_query),
):
    """
    Streaming web search using Perplexity Sonar only
    """

    async def generate():
        try:
            yield 'data: {"type": "web_search_start"}\n\n'

            # Get Sonar response with citations
            answer, citations = await get_sonar_answer(question)

            # Stream the answer text
            for char in answer:
                data = {
                    "type": "answer_chunk",
                    "content": char,
                }
                yield f"data: {json.dumps(data)}\n\n"

            # Send citations
            if citations:
                yield f"data: {
                    json.dumps({'type': 'citations', 'citations': citations})
                }\n\n"

            yield 'data: {"type": "web_search_end"}\n\n'

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")


# Case Search (Qdrant + GPT) Only Endpoints
@router.post("/case-search", response_model=CaseSearchResponse)
async def case_search(
    request: QueryRequest, api_key_valid: bool = Depends(verify_api_key)
):
    """
    Case search using Qdrant + GPT only
    Returns answer based on court cases without web search
    """
    try:
        # Get supporting cases from Qdrant
        supporting_cases = await get_cases_from_qdrant(request.question, request.top_k)

        # Generate case-based answer
        answer = ""
        if supporting_cases:
            client = get_openai_client()
            answer = await answer_based_on_cases(
                request.question, supporting_cases, client
            )

        return CaseSearchResponse(
            answer=answer,
            supporting_cases=supporting_cases,
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/case-search-stream")
async def case_search_stream(
    question: str = Query(..., description="Legal question to search"),
    top_k: int = Query(5, description="Number of cases to retrieve"),
    api_key_valid: bool = Depends(verify_api_key_query),
):
    """
    Streaming case search using Qdrant + GPT only
    """

    async def generate():
        try:
            yield 'data: {"type": "case_search_start"}\n\n'

            client = get_openai_client()

            yield 'data: {"type": "cases_fetching"}\n\n'

            supporting_cases = await get_cases_from_qdrant(question, top_k)

            yield 'data: {"type": "gpt_answer_start"}\n\n'

            if supporting_cases:
                async for chunk in answer_based_on_cases_stream(
                    question, supporting_cases, client
                ):
                    data = {
                        "type": "answer_chunk",
                        "content": chunk,
                    }
                    yield f"data: {json.dumps(data)}\n\n"

            yield 'data: {"type": "gpt_answer_end"}\n\n'

            yield 'data: {"type": "cases_start"}\n\n'

            for case in supporting_cases:
                case_data = {
                    "type": "case",
                    "case_number": case.case_number,
                    "court": case.court,
                    "judge": case.judge,
                    "subject": case.subject,
                    "date_issued": case.date_issued,
                    "ecli": case.ecli,
                    "keywords": case.keywords,
                    "legal_references": case.legal_references,
                    "relevance_score": round(case.relevance_score, 3),
                    "source_url": case.source_url,
                }
                yield f"data: {json.dumps(case_data)}\n\n"

            yield 'data: {"type": "case_search_end"}\n\n'

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")


# Combined Search (Web + Case) Endpoints
@router.post("/combined-search", response_model=CombinedSearchResponse)
async def combined_search(
    request: QueryRequest, api_key_valid: bool = Depends(verify_api_key)
):
    """
    Combined search using both web (Sonar) and case (Qdrant + GPT) sources
    Returns answers from both sources with citations and case information
    """
    try:
        # Get Sonar answer with citations
        web_answer, web_citations = await get_sonar_answer(request.question)

        # Get supporting cases from Qdrant
        supporting_cases = await get_cases_from_qdrant(request.question, request.top_k)

        # Generate case-based answer
        case_answer = ""
        if supporting_cases:
            client = get_openai_client()
            case_answer = await answer_based_on_cases(
                request.question, supporting_cases, client
            )

        return CombinedSearchResponse(
            web_answer=web_answer,
            web_source="Perplexity Sonar via OpenRouter",
            web_citations=web_citations,
            case_answer=case_answer,
            supporting_cases=supporting_cases,
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/combined-search-stream")
async def combined_search_stream(
    question: str = Query(..., description="Legal question to search"),
    top_k: int = Query(5, description="Number of cases to retrieve"),
    api_key_valid: bool = Depends(verify_api_key_query),
):
    """
    Streaming combined search using both web (Sonar) and case (Qdrant + GPT) sources
    """

    async def generate():
        try:
            client = get_openai_client()

            # Web Search Part
            yield 'data: {"type": "web_search_start"}\n\n'

            # Get Sonar response with citations
            web_answer, web_citations = await get_sonar_answer(question)

            # Stream the web answer text
            for char in web_answer:
                data = {
                    "type": "web_answer_chunk",
                    "content": char,
                }
                yield f"data: {json.dumps(data)}\n\n"

            # Send web citations
            if web_citations:
                yield f"data: {
                    json.dumps({'type': 'web_citations', 'citations': web_citations})
                }\n\n"

            yield 'data: {"type": "web_search_end"}\n\n'

            # Case Search Part
            yield 'data: {"type": "case_search_start"}\n\n'

            yield 'data: {"type": "cases_fetching"}\n\n'

            supporting_cases = await get_cases_from_qdrant(question, top_k)

            yield 'data: {"type": "gpt_answer_start"}\n\n'

            if supporting_cases:
                async for chunk in answer_based_on_cases_stream(
                    question, supporting_cases, client
                ):
                    data = {
                        "type": "case_answer_chunk",
                        "content": chunk,
                    }
                    yield f"data: {json.dumps(data)}\n\n"

            yield 'data: {"type": "gpt_answer_end"}\n\n'

            yield 'data: {"type": "cases_start"}\n\n'

            for case in supporting_cases:
                case_data = {
                    "type": "case",
                    "case_number": case.case_number,
                    "court": case.court,
                    "judge": case.judge,
                    "subject": case.subject,
                    "date_issued": case.date_issued,
                    "ecli": case.ecli,
                    "keywords": case.keywords,
                    "legal_references": case.legal_references,
                    "relevance_score": round(case.relevance_score, 3),
                    "source_url": case.source_url,
                }
                yield f"data: {json.dumps(case_data)}\n\n"

            yield 'data: {"type": "combined_search_end"}\n\n'

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")
