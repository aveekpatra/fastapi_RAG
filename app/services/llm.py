from openai import OpenAI

from app.config import settings
from app.models import CaseResult
from app.utils.formatters import format_cases_for_context

SYSTEM_PROMPT = """Jste prÃ¡vnÃ­ analytik specializujÃ­cÃ­ se na ÄeskÃ© prÃ¡vo. VaÅ¡Ã­m Ãºkolem je analyzovat poskytnutÃ¡ soudnÃ­ rozhodnutÃ­ a odpovÄ›dÄ›t na otÃ¡zku uÅ¾ivatele.

KRITICKÃ PRAVIDLA - ABSOLUTNÃ ZÃKAZ HALUCINACÃ:
1. PouÅ¾Ã­vejte POUZE informace z poskytnutÃ½ch rozhodnutÃ­
2. NIKDY nevymÃ½Å¡lejte prÃ¡vnÃ­ zÃ¡vÄ›ry, kterÃ© nejsou v rozhodnutÃ­ch
3. Pokud rozhodnutÃ­ neobsahujÃ­ odpovÄ›Ä, JASNÄš to Å™eknÄ›te
4. NIKDY neodkazujte na zÃ¡kony nebo paragrafy, kterÃ© nejsou zmÃ­nÄ›ny v rozhodnutÃ­ch
5. Citujte POUZE skuteÄnÃ© ÄÃ¡sti z poskytnutÃ½ch rozhodnutÃ­

FORMÃT ODPOVÄšDI:

**ShrnutÃ­ relevance:**
Nejprve v 1-2 vÄ›tÃ¡ch Å™eknÄ›te, zda poskytnutÃ¡ rozhodnutÃ­ odpovÃ­dajÃ­ na otÃ¡zku, nebo ne.

**AnalÃ½za rozhodnutÃ­:**
Pro KAÅ½DÃ‰ relevantnÃ­ rozhodnutÃ­ uveÄte:

ğŸ“‹ **[SpisovÃ¡ znaÄka]** - [Soud], [Datum]
- **Co Å™eÅ¡ilo:** [StruÄnÃ½ popis pÅ™Ã­padu z rozhodnutÃ­]
- **KlÃ­ÄovÃ© zÃ¡vÄ›ry:** [KonkrÃ©tnÃ­ zÃ¡vÄ›ry soudu z rozhodnutÃ­]
- **PrÃ¡vnÃ­ pÅ™edpisy:** [Pouze ty, kterÃ© jsou zmÃ­nÄ›ny v rozhodnutÃ­]
- **Relevance pro vaÅ¡i otÃ¡zku:** [Jak se to vztahuje k otÃ¡zce]

**OdpovÄ›Ä na otÃ¡zku:**
Na zÃ¡kladÄ› analyzovanÃ½ch rozhodnutÃ­ [odpovÄ›Ä]. Citujte konkrÃ©tnÃ­ rozhodnutÃ­ inline pomocÃ­ [^1], [^2] atd.

**Pokud rozhodnutÃ­ neodpovÃ­dajÃ­:**
Pokud poskytnutÃ¡ rozhodnutÃ­ neobsahujÃ­ odpovÄ›Ä na otÃ¡zku, napiÅ¡te:
"âš ï¸ PoskytnutÃ¡ rozhodnutÃ­ se nezabÃ½vajÃ­ [tÃ©matem otÃ¡zky]. Pro odpovÄ›Ä na tuto otÃ¡zku by bylo potÅ™eba nalÃ©zt rozhodnutÃ­ tÃ½kajÃ­cÃ­ se [konkrÃ©tnÃ­ tÃ©ma]."

INLINE CITACE:
- PouÅ¾Ã­vejte [^1], [^2], [^3] pro odkazy na konkrÃ©tnÃ­ rozhodnutÃ­
- Na konci odpovÄ›di uveÄte seznam citacÃ­:

**CitovanÃ© pÅ™Ã­pady:**
[^1]: [SpisovÃ¡ znaÄka], [Soud], [Datum], ECLI: [ECLI]
[^2]: [SpisovÃ¡ znaÄka], [Soud], [Datum], ECLI: [ECLI]

PÅ˜ÃKLAD DOBRÃ‰ ODPOVÄšDI:
"Podle rozhodnutÃ­ NejvyÅ¡Å¡Ã­ho soudu [^1] platÃ­, Å¾e [konkrÃ©tnÃ­ zÃ¡vÄ›r z rozhodnutÃ­]. Toto bylo potvrzeno i v pÅ™Ã­padÄ› [^2], kde soud rozhodl, Å¾e [konkrÃ©tnÃ­ zÃ¡vÄ›r]."

PÅ˜ÃKLAD Å PATNÃ‰ ODPOVÄšDI (HALUCINACE):
"Podle Â§ 123 zÃ¡kona XYZ..." (pokud tento paragraf nenÃ­ v rozhodnutÃ­ch)
"ObecnÄ› platÃ­, Å¾e..." (bez odkazu na konkrÃ©tnÃ­ rozhodnutÃ­)
"Soud by pravdÄ›podobnÄ› rozhodl..." (spekulace)

PAMATUJTE: RadÄ›ji Å™eknÄ›te "nevÃ­m" neÅ¾ vymÃ½Å¡lejte informace!"""

SONAR_PROMPT = """Jste prÃ¡vnÃ­ expert se specialistem na ÄeskÃ© prÃ¡vo. OdpovÃ­dejte na otÃ¡zky uÅ¾ivatele VÃHRADNÄš na zÃ¡kladÄ› poskytnutÃ½ch rozhodnutÃ­ ÄeskÃ½ch soudÅ¯.

VaÅ¡e odpovÄ›Ä musÃ­ obsahovat:
1. PÅ™Ã­mou odpovÄ›Ä na poloÅ¾enou otÃ¡zku na zÃ¡kladÄ› pÅ™Ã­sluÅ¡nÃ½ch rozhodnutÃ­
2. Citace vÅ¡ech relevantnÃ­ch, aktuÃ¡lnÃ­ch a konkrÃ©tnÃ­ch zÃ¡konÅ¯, vyhlÃ¡Å¡ek, prÃ¡vnÃ­ch pÅ™edpisÅ¯, prÃ¡vnÃ­ch principÅ¯, zrÃ¡tka zÃ¡kona, musÃ­ obsahovat:
   - KonkrÃ©tnÃ­ paragraf a ÄÃ­slo zÃ¡konu
   - Datum vydÃ¡nÃ­
   - Datum vydÃ¡nÃ­
   - ECLI reference
   - RelevantnÃ­ prÃ¡vnÃ­ pÅ™edpisy (Â§ citace)

OdpovÄ›Ä musÃ­ bÃ½t:
- StrukturovanÃ¡ a logickÃ¡
- PsanÃ¡ v ÄeÅ¡tinÄ›
- SoustÅ™edÄ›na vÃ½hradnÄ› na poskytnutÃ© informace
- Bez generalizacÃ­ nebo informacÃ­ mimo zÃ¡kladnu rozhodnutÃ­
- S pÅ™esnÃ½mi citacemi a odkazem
- MusÃ­ vychÃ¡zet z kontextu, musÃ­ brÃ¡t v potaz i prÃ¡vnÃ­ principy, strukturu a hierarchii zÃ¡konÅ¯
- PouÅ¾Ã­vejte pouze Ãºdaje z oficiÃ¡lnÃ­ch vlÃ¡dnÃ­ch nebo renomovanÃ½ch prÃ¡vnÃ­ch webÅ¯ (napÅ™. zakonyprolidi.cz, nsoud.cz, eur-lex.europa.eu)
- VyhÃ½bejte se citacÃ­m z nÃ¡hodnÃ½ch fÃ³r, diskuznÃ­ch skupin nebo uÅ¾ivatelskÃ½ch komentÃ¡Å™Å¯

Pokud je otÃ¡zka nezodpovÄ›ditelnÃ¡ na zÃ¡kladÄ› tÄ›chto dat a tohoto postupu, vÃ½slovnÄ› to uveÄte."""


def get_openai_client() -> OpenAI:
    """Get configured OpenAI client for OpenRouter"""
    return OpenAI(
        api_key=settings.OPENROUTER_API_KEY,
        base_url=settings.OPENROUTER_BASE_URL,
    )


async def get_sonar_answer(question: str) -> tuple[str, list[str]]:
    """
    Get answer from Perplexity Sonar with citations
    Returns: (answer_text, citations_list)
    """
    try:
        client = get_openai_client()

        sonar_response = client.chat.completions.create(
            model="perplexity/sonar",
            messages=[
                {"role": "system", "content": SONAR_PROMPT},
                {"role": "user", "content": question},
            ],
            stream=False,
        )

        sonar_answer = sonar_response.choices[0].message.content or ""

        # Capture citations
        sonar_citations = getattr(sonar_response, "citations", [])
        if not sonar_citations:
            search_results = getattr(sonar_response, "search_results", [])
            sonar_citations = [
                result.get("url", "") for result in search_results if result.get("url")
            ]

        return sonar_answer, sonar_citations

    except Exception as e:
        print(f"Chyba pri ziskani Sonar odpovedi: {str(e)}")
        return "", []


async def get_sonar_answer_stream(question: str):
    """
    Get streaming answer from Perplexity Sonar with citations
    Yields: (chunk_text, final_answer, citations_list)
    
    Note: Perplexity's streaming API doesn't include citations in chunks.
    We need to make a separate non-streaming call to get citations.
    """
    try:
        client = get_openai_client()

        # Start streaming the answer
        stream = client.chat.completions.create(
            model="perplexity/sonar",
            messages=[
                {"role": "system", "content": SONAR_PROMPT},
                {"role": "user", "content": question},
            ],
            stream=True,
        )

        full_answer = ""
        citations = []

        # Stream the content
        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_answer += content
                yield content, None, None

        # After streaming completes, make a non-streaming call to get citations
        # This is necessary because Perplexity's streaming API doesn't include citations
        try:
            citation_response = client.chat.completions.create(
                model="perplexity/sonar",
                messages=[
                    {"role": "system", "content": SONAR_PROMPT},
                    {"role": "user", "content": question},
                ],
                stream=False,
            )
            
            # Extract citations from the response
            citations = getattr(citation_response, "citations", [])
            if not citations:
                # Fallback to search_results if citations not available
                search_results = getattr(citation_response, "search_results", [])
                citations = [
                    result.get("url", "") for result in search_results if result.get("url")
                ]
        except Exception as citation_error:
            print(f"Error fetching citations: {str(citation_error)}")
            citations = []

        # Final yield with complete answer and citations
        yield None, full_answer, citations

    except Exception as e:
        print(f"Chyba pri ziskani Sonar odpovedi: {str(e)}")
        yield None, "", []


async def answer_based_on_cases(
    question: str, cases: list[CaseResult], client: OpenAI
) -> str:
    """
    GPT-4o answers the question based on all case data with citations
    """
    try:
        cases_context = format_cases_for_context(cases)

        response = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {
                    "role": "user",
                    "content": f"""OTÃZKA UÅ½IVATELE:
{question}

POSKYTNUTÃ SOUDNÃ ROZHODNUTÃ:
{cases_context}

ÃšKOL:
1. Analyzujte kaÅ¾dÃ© rozhodnutÃ­ a zjistÄ›te, zda obsahuje informace relevantnÃ­ k otÃ¡zce
2. Pokud ANO: VytvoÅ™te strukturovanou odpovÄ›Ä s inline citacemi [^1], [^2] atd.
3. Pokud NE: JasnÄ› Å™eknÄ›te, Å¾e rozhodnutÃ­ se netÃ½kajÃ­ tÃ©to otÃ¡zky
4. NIKDY nevymÃ½Å¡lejte informace, kterÃ© nejsou v rozhodnutÃ­ch
5. Citujte konkrÃ©tnÃ­ ÄÃ¡sti rozhodnutÃ­, ne obecnÃ© prÃ¡vnÃ­ znalosti

ZaÄnÄ›te analÃ½zou relevance rozhodnutÃ­.""",
                },
            ],
            temperature=0.3,  # SnÃ­Å¾enÃ¡ teplota pro menÅ¡Ã­ halucinace
            max_tokens=2500,
        )

        answer = (response.choices[0].message.content or "").strip()
        return answer

    except Exception as e:
        print(f"Chyba pri generovani odpovedi zalozene na pripadech: {str(e)}")
        return ""


async def answer_based_on_cases_stream(
    question: str, cases: list[CaseResult], client: OpenAI
):
    """
    Stream GPT-4o answer based on cases
    """
    try:
        print(f"ğŸ“ Formatting {len(cases)} cases for context...")
        cases_context = format_cases_for_context(cases)
        print(f"ğŸ“ Context length: {len(cases_context)} characters")

        print(f"ğŸ¤– Starting OpenAI streaming...")
        stream = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {
                    "role": "user",
                    "content": f"""OTÃZKA UÅ½IVATELE:
{question}

POSKYTNUTÃ SOUDNÃ ROZHODNUTÃ:
{cases_context}

ÃšKOL:
1. Analyzujte kaÅ¾dÃ© rozhodnutÃ­ a zjistÄ›te, zda obsahuje informace relevantnÃ­ k otÃ¡zce
2. Pokud ANO: VytvoÅ™te strukturovanou odpovÄ›Ä s inline citacemi [^1], [^2] atd.
3. Pokud NE: JasnÄ› Å™eknÄ›te, Å¾e rozhodnutÃ­ se netÃ½kajÃ­ tÃ©to otÃ¡zky
4. NIKDY nevymÃ½Å¡lejte informace, kterÃ© nejsou v rozhodnutÃ­ch
5. Citujte konkrÃ©tnÃ­ ÄÃ¡sti rozhodnutÃ­, ne obecnÃ© prÃ¡vnÃ­ znalosti

ZaÄnÄ›te analÃ½zou relevance rozhodnutÃ­.""",
                },
            ],
            temperature=0.3,  # SnÃ­Å¾enÃ¡ teplota pro menÅ¡Ã­ halucinace
            max_tokens=2500,
            stream=True,
        )

        chunk_count = 0
        for chunk in stream:
            if chunk.choices[0].delta.content:
                chunk_count += 1
                content = chunk.choices[0].delta.content
                yield content
        
        print(f"âœ… Yielded {chunk_count} chunks from OpenAI")
        
        if chunk_count == 0:
            print("âš ï¸ WARNING: OpenAI returned 0 chunks!")

    except Exception as e:
        print(f"âŒ Chyba pri streamovani odpovedi: {str(e)}")
        import traceback
        traceback.print_exc()
        traceback.print_exc()



async def generate_combined_summary_stream(
    question: str,
    web_answer: str,
    case_answer: str,
    client: OpenAI
):
    """
    Generate a concise summary combining web and case search results
    """
    try:
        summary_prompt = """Jste prÃ¡vnÃ­ expert. MÃ¡te k dispozici dvÄ› odpovÄ›di na stejnou otÃ¡zku:
1. OdpovÄ›Ä z webovÃ©ho vyhledÃ¡vÃ¡nÃ­ (aktuÃ¡lnÃ­ prÃ¡vnÃ­ informace)
2. OdpovÄ›Ä zaloÅ¾enÃ¡ na soudnÃ­ch rozhodnutÃ­ch (judikatura)

VytvoÅ™te KRÃTKÃ‰ shrnutÃ­ (2-3 vÄ›ty), kterÃ©:
- Syntetizuje obÄ› odpovÄ›di
- ZdÅ¯raznÃ­ klÃ­ÄovÃ© body
- UkÃ¡Å¾e, jak se webovÃ© informace a judikatura doplÅˆujÃ­
- BuÄte struÄnÃ½ a jasnÃ½

NEOPISUJTE celÃ© odpovÄ›di, pouze shrÅˆte hlavnÃ­ zÃ¡vÄ›ry."""

        stream = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[
                {"role": "system", "content": summary_prompt},
                {
                    "role": "user",
                    "content": f"""OTÃZKA:
{question}

WEBOVÃ ODPOVÄšÄ:
{web_answer[:1000]}

ODPOVÄšÄ ZE SOUDNÃCH ROZHODNUTÃ:
{case_answer[:1000]}

VytvoÅ™te krÃ¡tkÃ© shrnutÃ­ (2-3 vÄ›ty):"""
                }
            ],
            temperature=0.3,
            max_tokens=300,
            stream=True,
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    except Exception as e:
        print(f"Error generating summary: {str(e)}")
        yield ""
