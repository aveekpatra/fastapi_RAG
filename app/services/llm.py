from openai import OpenAI

from app.config import settings
from app.models import CaseResult
from app.utils.formatters import format_cases_for_context

SYSTEM_PROMPT = """Jste prÃ¡vnÃ­ analytik specializujÃ­cÃ­ se na ÄeskÃ© prÃ¡vo. VaÅ¡Ã­m Ãºkolem je analyzovat soudnÃ­ rozhodnutÃ­ a odpovÄ›dÄ›t na otÃ¡zku uÅ¾ivatele pÅ™irozenÃ½m zpÅ¯sobem s citacemi.

KRITICKÃ PRAVIDLA:
1. PouÅ¾Ã­vejte POUZE informace z poskytnutÃ½ch rozhodnutÃ­
2. Extrahujte KONKRÃ‰TNÃ zÃ¡vÄ›ry z ODÅ®VODNÄšNÃ rozhodnutÃ­
3. Citujte DOSLOVNÄš klÃ­ÄovÃ© pasÃ¡Å¾e z odÅ¯vodnÄ›nÃ­
4. **Pokud rozhodnutÃ­ NEJSOU relevantnÃ­, zaÄnÄ›te odpovÄ›Ä pÅ™esnÄ› slovy: "âš ï¸ Å½ÃDNÃ‰ RELEVANTNÃ PÅ˜ÃPADY"**
5. NIKDY nevymÃ½Å¡lejte informace

FORMÃT ODPOVÄšDI:

NapiÅ¡te pÅ™irozenou, plynulou odpovÄ›Ä na otÃ¡zku, kterÃ¡:

1. **PÅ™Ã­mo odpovÃ­dÃ¡ na otÃ¡zku** - zaÄnÄ›te odpovÄ›dÃ­, ne analÃ½zou
2. **PouÅ¾Ã­vÃ¡ inline citace** [^1], [^2] pro kaÅ¾dÃ© tvrzenÃ­
3. **Cituje konkrÃ©tnÃ­ zÃ¡vÄ›ry z odÅ¯vodnÄ›nÃ­** - ne jen tÃ©mata
4. **VysvÄ›tluje PROÄŒ soud rozhodl tak, jak rozhodl** - pouÅ¾ijte ÄÃ¡st "odÅ¯vodnÄ›nÃ­"

**Struktura:**

[PÅ™Ã­mÃ¡ odpovÄ›Ä na otÃ¡zku s citacemi]

Podle rozhodnutÃ­ [^1], [konkrÃ©tnÃ­ zÃ¡vÄ›r soudu z odÅ¯vodnÄ›nÃ­]. Soud v odÅ¯vodnÄ›nÃ­ uvedl, Å¾e "[doslovnÃ¡ citace z odÅ¯vodnÄ›nÃ­]". 

V pÅ™Ã­padÄ› [^2], soud dospÄ›l k zÃ¡vÄ›ru, Å¾e [konkrÃ©tnÃ­ zÃ¡vÄ›r]. OdÅ¯vodnÄ›nÃ­ zdÅ¯raznilo, Å¾e "[doslovnÃ¡ citace]".

[DalÅ¡Ã­ pÅ™Ã­pady s konkrÃ©tnÃ­mi zÃ¡vÄ›ry...]

**Co jsme se nauÄili z tÄ›chto pÅ™Ã­padÅ¯:**

- [KonkrÃ©tnÃ­ pouÄenÃ­ 1 z odÅ¯vodnÄ›nÃ­]
- [KonkrÃ©tnÃ­ pouÄenÃ­ 2 z odÅ¯vodnÄ›nÃ­]
- [KonkrÃ©tnÃ­ pouÄenÃ­ 3 z odÅ¯vodnÄ›nÃ­]

**CitovanÃ© pÅ™Ã­pady:**
[^1]: [[SpisovÃ¡ znaÄka]](URL) - [Soud], [Datum], ECLI: [ECLI]
[^2]: [[SpisovÃ¡ znaÄka]](URL) - [Soud], [Datum], ECLI: [ECLI]

DÅ®LEÅ½ITÃ‰: VytvoÅ™te KLIKATELNÃ‰ odkazy ve formÃ¡tu Markdown:
- PouÅ¾ijte: [[SpisovÃ¡ znaÄka]](URL)
- URL najdete v ÄÃ¡sti "ZDROJ" kaÅ¾dÃ©ho rozhodnutÃ­
- PÅ™Ã­klad: [[8 C 171/2023-103]](https://rozhodnuti.justice.cz/api/finaldoc/abc123)

---

PÅ˜ÃKLAD DOBRÃ‰ ODPOVÄšDI:

"ManÅ¾elÃ© s nezletilÃ½mi dÄ›tmi musÃ­ pÅ™i rozvodu uzavÅ™Ã­t dohodu o ÃºpravÄ› pomÄ›rÅ¯ k dÄ›tem [^1]. Podle rozhodnutÃ­ OkresnÃ­ho soudu v Praze, tato dohoda musÃ­ obsahovat konkrÃ©tnÃ­ Ãºpravu vÃ½Å¾ivnÃ©ho, bydlenÃ­ dÃ­tÄ›te a vÃ½konu rodiÄovskÃ© odpovÄ›dnosti [^1]. Soud v odÅ¯vodnÄ›nÃ­ zdÅ¯raznil, Å¾e 'bez pÅ™edloÅ¾enÃ­ ÃºplnÃ© a schvÃ¡lenÃ© dohody nelze rozvod manÅ¾elstvÃ­ vyslovit, neboÅ¥ zÃ¡kon chrÃ¡nÃ­ zÃ¡jmy nezletilÃ½ch dÄ›tÃ­' [^1].

V pÅ™Ã­padÄ› [^2] soud odmÃ­tl nÃ¡vrh na rozvod, protoÅ¾e pÅ™edloÅ¾enÃ¡ dohoda neobsahovala konkrÃ©tnÃ­ ÄÃ¡stku vÃ½Å¾ivnÃ©ho. V odÅ¯vodnÄ›nÃ­ soud uvedl, Å¾e 'neurÄitÃ¡ formulace typu 'pÅ™imÄ›Å™enÃ© vÃ½Å¾ivnÃ©' nenÃ­ dostaÄujÃ­cÃ­, dohoda musÃ­ obsahovat pÅ™esnou ÄÃ¡stku a periodicitu plateb' [^2].

**Co jsme se nauÄili:**
- Dohoda musÃ­ bÃ½t konkrÃ©tnÃ­ a ÃºplnÃ¡, ne obecnÃ¡
- MusÃ­ obsahovat: vÃ½Å¾ivnÃ© (ÄÃ¡stka + periodicita), bydlenÃ­ dÃ­tÄ›te, vÃ½kon rodiÄovskÃ© odpovÄ›dnosti
- Bez schvÃ¡lenÃ© dohody soud rozvod nevyslovÃ­

**CitovanÃ© pÅ™Ã­pady:**
[^1]: [[25 Cdo 1234/2020]](https://rozhodnuti.justice.cz/api/finaldoc/abc123) - NejvyÅ¡Å¡Ã­ soud, 2020-05-15, ECLI: ECLI:CZ:NS:2020:25.CDO.1234.2020.1
[^2]: [[10 C 567/2019]](https://rozhodnuti.justice.cz/api/finaldoc/def456) - OkresnÃ­ soud v Praze, 2019-11-20, ECLI: ECLI:CZ:OSPH:2019:10.C.567.2019.1"

---

PÅ˜ÃKLAD Å PATNÃ‰ ODPOVÄšDI:

"RozhodnutÃ­ se zabÃ½vajÃ­ rodiÄovskou odpovÄ›dnostÃ­ [^1], [^2], [^3]. Soudy Å™eÅ¡ily vÃ½Å¾ivnÃ© a vÃ½chovu dÄ›tÃ­." âŒ

PROÄŒ JE Å PATNÃ:
- NeÅ™Ã­kÃ¡, CO KONKRÃ‰TNÄš soudy rozhodly
- ChybÃ­ citace z odÅ¯vodnÄ›nÃ­
- NeodpovÃ­dÃ¡ pÅ™Ã­mo na otÃ¡zku
- NenÃ­ jasnÃ©, co se z pÅ™Ã­padÅ¯ nauÄÃ­me

---

PAMATUJTE:
- PiÅ¡te jako prÃ¡vnÃ­k vysvÄ›tlujÃ­cÃ­ klientovi, ne jako robot
- KaÅ¾dÃ© tvrzenÃ­ = citace
- Citujte z ODÅ®VODNÄšNÃ, ne jen z vÃ½roku
- VysvÄ›tlete PROÄŒ soud rozhodl tak, jak rozhodl
- BuÄte konkrÃ©tnÃ­: ÄÃ¡stky, data, podmÃ­nky, kritÃ©ria"""

SONAR_PROMPT = """Jste prÃ¡vnÃ­ expert specializujÃ­cÃ­ se na ÄeskÃ© prÃ¡vo a LEGISLATIVU. OdpovÃ­dejte na otÃ¡zky uÅ¾ivatele na zÃ¡kladÄ› AKTUÃLNÃCH ZÃKONÅ®, VYHLÃÅ EK a PRÃVNÃCH PÅ˜EDPISÅ®.

KRITICKY DÅ®LEÅ½ITÃ‰:
- VyhledÃ¡vejte POUZE v LEGISLATIVÄš (zÃ¡kony, vyhlÃ¡Å¡ky, naÅ™Ã­zenÃ­)
- NEVYHLEDÃVEJTE v judikatuÅ™e nebo soudnÃ­ch rozhodnutÃ­ch
- ZamÄ›Å™te se na oficiÃ¡lnÃ­ prÃ¡vnÃ­ pÅ™edpisy, ne na soudnÃ­ praxi

VaÅ¡e odpovÄ›Ä musÃ­ obsahovat:
1. PÅ™Ã­mou odpovÄ›Ä na otÃ¡zku zaloÅ¾enou na AKTUÃLNÃ LEGISLATIVÄš
2. Citace konkrÃ©tnÃ­ch zÃ¡konÅ¯ a vyhlÃ¡Å¡ek:
   - KonkrÃ©tnÃ­ paragraf a ÄÃ­slo zÃ¡konu (napÅ™. Â§ 123 zÃ¡kona Ä. 89/2012 Sb.)
   - NÃ¡zev zÃ¡kona
   - Datum ÃºÄinnosti (pokud je relevantnÃ­)
   - Odkaz na oficiÃ¡lnÃ­ zdroj (napÅ™. zakonyprolidi.cz)

OdpovÄ›Ä musÃ­ bÃ½t:
- StrukturovanÃ¡ a logickÃ¡
- PsanÃ¡ v ÄeÅ¡tinÄ›
- ZaloÅ¾enÃ¡ VÃHRADNÄš na legislativÄ›, NE na judikatuÅ™e
- S pÅ™esnÃ½mi citacemi paragrafÅ¯ a zÃ¡konÅ¯
- S odkazy na oficiÃ¡lnÃ­ zdroje (zakonyprolidi.cz, psp.cz, eur-lex.europa.eu)

VYHÃBEJTE SE:
- CitacÃ­m soudnÃ­ch rozhodnutÃ­ (to je pro jinÃ½ typ vyhledÃ¡vÃ¡nÃ­)
- OdkazÅ¯m na judikÃ¡ty nebo ECLI
- WebÅ¯m s judikaturou (nsoud.cz, justice.cz)

PREFERUJTE:
- OficiÃ¡lnÃ­ znÄ›nÃ­ zÃ¡konÅ¯
- VlÃ¡dnÃ­ a parlamentnÃ­ zdroje
- OficiÃ¡lnÃ­ prÃ¡vnÃ­ databÃ¡ze legislativy
- MusÃ­ vychÃ¡zet z kontextu, musÃ­ brÃ¡t v potaz i prÃ¡vnÃ­ principy, strukturu a hierarchii zÃ¡konÅ¯
- PouÅ¾Ã­vejte pouze Ãºdaje z oficiÃ¡lnÃ­ch vlÃ¡dnÃ­ch nebo renomovanÃ½ch prÃ¡vnÃ­ch webÅ¯ (napÅ™. zakonyprolidi.cz, nsoud.cz, eur-lex.europa.eu)
- VyhÃ½bejte se citacÃ­m z nÃ¡hodnÃ½ch fÃ³r, diskuznÃ­ch skupin nebo uÅ¾ivatelskÃ½ch komentÃ¡Å™Å¯

Pokud je otÃ¡zka nezodpovÄ›ditelnÃ¡ na zÃ¡kladÄ› tÄ›chto dat a tohoto postupu, vÃ½slovnÄ› to uveÄte."""


def get_openai_client() -> OpenAI:
    """Get configured OpenAI client for OpenRouter"""
    return OpenAI(
        api_key=settings.OPENROUTER_API_KEY,
        base_url=settings.OPENROUTER_BASE_URL,
    )


async def get_sonar_answer(question: str) -> tuple[str, list[str]]:
    """
    Get answer from Perplexity Sonar with citations
    Returns: (answer_text, citations_list)
    """
    try:
        client = get_openai_client()

        sonar_response = client.chat.completions.create(
            model="perplexity/sonar",
            messages=[
                {"role": "system", "content": SONAR_PROMPT},
                {"role": "user", "content": question},
            ],
            stream=False,
        )

        sonar_answer = sonar_response.choices[0].message.content or ""

        # Capture citations
        sonar_citations = getattr(sonar_response, "citations", [])
        if not sonar_citations:
            search_results = getattr(sonar_response, "search_results", [])
            sonar_citations = [
                result.get("url", "") for result in search_results if result.get("url")
            ]

        return sonar_answer, sonar_citations

    except Exception as e:
        print(f"Chyba pri ziskani Sonar odpovedi: {str(e)}")
        return "", []


async def get_sonar_answer_stream(question: str):
    """
    Get streaming answer from Perplexity Sonar with citations
    Yields: (chunk_text, final_answer, citations_list)
    
    Note: Perplexity's streaming API doesn't include citations in chunks.
    We need to make a separate non-streaming call to get citations.
    """
    try:
        client = get_openai_client()

        # Start streaming the answer
        stream = client.chat.completions.create(
            model="perplexity/sonar",
            messages=[
                {"role": "system", "content": SONAR_PROMPT},
                {"role": "user", "content": question},
            ],
            stream=True,
        )

        full_answer = ""
        citations = []

        # Stream the content
        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_answer += content
                yield content, None, None

        # After streaming completes, make a non-streaming call to get citations
        # This is necessary because Perplexity's streaming API doesn't include citations
        try:
            citation_response = client.chat.completions.create(
                model="perplexity/sonar",
                messages=[
                    {"role": "system", "content": SONAR_PROMPT},
                    {"role": "user", "content": question},
                ],
                stream=False,
            )
            
            # Extract citations from the response
            citations = getattr(citation_response, "citations", [])
            if not citations:
                # Fallback to search_results if citations not available
                search_results = getattr(citation_response, "search_results", [])
                citations = [
                    result.get("url", "") for result in search_results if result.get("url")
                ]
        except Exception as citation_error:
            print(f"Error fetching citations: {str(citation_error)}")
            citations = []

        # Final yield with complete answer and citations
        yield None, full_answer, citations

    except Exception as e:
        print(f"Chyba pri ziskani Sonar odpovedi: {str(e)}")
        yield None, "", []


async def answer_based_on_cases(
    question: str, cases: list[CaseResult], client: OpenAI
) -> str:
    """
    GPT-4o answers the question based on FULL case data with citations
    NO TRUNCATION - All context is passed to GPT
    """
    try:
        # Format cases with FULL context - NO TRUNCATION
        cases_context = format_cases_for_context(cases)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“¤ PASSING FULL CONTEXT TO GPT")
        print(f"{'='*80}")
        print(f"Number of cases: {len(cases)}")
        print(f"Context length: {len(cases_context)} characters")
        print(f"Context length: {len(cases_context.split())} words")
        print(f"Estimated tokens: ~{len(cases_context) // 4}")
        print(f"{'='*80}\n")

        user_prompt = f"""OTÃZKA UÅ½IVATELE:
{question}

POSKYTNUTÃ SOUDNÃ ROZHODNUTÃ (KOMPLETNÃ KONTEXT):
{cases_context}

ÃšKOL:
1. PÅ™eÄtÄ›te si text kaÅ¾dÃ©ho rozhodnutÃ­ (ÄÃ¡st "PÅ˜EDMÄšT SPORU")
2. Extrahujte DOSLOVNÃ‰ CITACE z textu, kterÃ© odpovÃ­dajÃ­ na otÃ¡zku
3. VysvÄ›tlete PROÄŒ soud rozhodl tak, jak rozhodl
4. NapiÅ¡te pÅ™irozenou odpovÄ›Ä s inline citacemi [^1], [^2]

PÅ˜ÃKLAD DOBRÃ‰ ODPOVÄšDI:
"Podle rozhodnutÃ­ [^1] musÃ­ dohoda obsahovat konkrÃ©tnÃ­ Ãºpravu vÃ½Å¾ivnÃ©ho. Soud uvedl, Å¾e 'neurÄitÃ¡ formulace nenÃ­ dostaÄujÃ­cÃ­, dohoda musÃ­ obsahovat pÅ™esnou ÄÃ¡stku a periodicitu plateb'. V pÅ™Ã­padÄ› [^2] soud odmÃ­tl dohodu, protoÅ¾e 'zÃ¡kon vyÅ¾aduje jasnÃ© vymezenÃ­ prÃ¡v a povinnostÃ­ obou rodiÄÅ¯'."

DÅ®LEÅ½ITÃ‰: Citujte DOSLOVNÄš z textu rozhodnutÃ­. Pokud v textu nenÃ­ dostatek detailÅ¯, Å™eknÄ›te to."""

        response = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,
            max_tokens=4000,
        )

        answer = (response.choices[0].message.content or "").strip()
        
        print(f"âœ… GPT response generated: {len(answer)} characters\n")
        
        return answer

    except Exception as e:
        print(f"âŒ Chyba pri generovani odpovedi zalozene na pripadech: {str(e)}")
        import traceback
        traceback.print_exc()
        return ""


async def answer_based_on_cases_stream(
    question: str, cases: list[CaseResult], client: OpenAI
):
    """
    Stream GPT-4o answer based on FULL case data - NO TRUNCATION
    """
    try:
        print(f"\n{'='*80}")
        print(f"ğŸ“¤ STREAMING FULL CONTEXT TO GPT")
        print(f"{'='*80}")
        print(f"Number of cases: {len(cases)}")
        
        # Format cases with FULL context - NO TRUNCATION
        cases_context = format_cases_for_context(cases)
        
        print(f"Context length: {len(cases_context)} characters")
        print(f"Context length: {len(cases_context.split())} words")
        print(f"Estimated tokens: ~{len(cases_context) // 4}")
        print(f"{'='*80}\n")

        user_prompt = f"""OTÃZKA UÅ½IVATELE:
{question}

POSKYTNUTÃ SOUDNÃ ROZHODNUTÃ (KOMPLETNÃ KONTEXT):
{cases_context}

ÃšKOL:
1. PÅ™eÄtÄ›te si text kaÅ¾dÃ©ho rozhodnutÃ­ (ÄÃ¡st "PÅ˜EDMÄšT SPORU")
2. Extrahujte DOSLOVNÃ‰ CITACE z textu, kterÃ© odpovÃ­dajÃ­ na otÃ¡zku
3. VysvÄ›tlete PROÄŒ soud rozhodl tak, jak rozhodl
4. NapiÅ¡te pÅ™irozenou odpovÄ›Ä s inline citacemi [^1], [^2]

PÅ˜ÃKLAD DOBRÃ‰ ODPOVÄšDI:
"Podle rozhodnutÃ­ [^1] musÃ­ dohoda obsahovat konkrÃ©tnÃ­ Ãºpravu vÃ½Å¾ivnÃ©ho. Soud uvedl, Å¾e 'neurÄitÃ¡ formulace nenÃ­ dostaÄujÃ­cÃ­, dohoda musÃ­ obsahovat pÅ™esnou ÄÃ¡stku a periodicitu plateb'. V pÅ™Ã­padÄ› [^2] soud odmÃ­tl dohodu, protoÅ¾e 'zÃ¡kon vyÅ¾aduje jasnÃ© vymezenÃ­ prÃ¡v a povinnostÃ­ obou rodiÄÅ¯'."

DÅ®LEÅ½ITÃ‰: Citujte DOSLOVNÄš z textu rozhodnutÃ­. Pokud v textu nenÃ­ dostatek detailÅ¯, Å™eknÄ›te to."""

        print(f"ğŸ¤– Starting OpenAI streaming...")
        stream = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,
            max_tokens=4000,
            stream=True,
        )

        chunk_count = 0
        for chunk in stream:
            if chunk.choices[0].delta.content:
                chunk_count += 1
                content = chunk.choices[0].delta.content
                yield content
        
        print(f"âœ… Yielded {chunk_count} chunks from OpenAI")
        
        if chunk_count == 0:
            print("âš ï¸ WARNING: OpenAI returned 0 chunks!")

    except Exception as e:
        print(f"âŒ Chyba pri streamovani odpovedi: {str(e)}")
        import traceback
        traceback.print_exc()



async def generate_combined_summary_stream(
    question: str,
    web_answer: str,
    case_answer: str,
    client: OpenAI
):
    """
    Generate a concise summary combining web and case search results
    """
    try:
        summary_prompt = """Jste prÃ¡vnÃ­ expert. MÃ¡te k dispozici dvÄ› odpovÄ›di na stejnou otÃ¡zku:
1. OdpovÄ›Ä z webovÃ©ho vyhledÃ¡vÃ¡nÃ­ (aktuÃ¡lnÃ­ prÃ¡vnÃ­ informace)
2. OdpovÄ›Ä zaloÅ¾enÃ¡ na soudnÃ­ch rozhodnutÃ­ch (judikatura)

VytvoÅ™te KRÃTKÃ‰ shrnutÃ­ (2-3 vÄ›ty), kterÃ©:
- Syntetizuje obÄ› odpovÄ›di
- ZdÅ¯raznÃ­ klÃ­ÄovÃ© body
- UkÃ¡Å¾e, jak se webovÃ© informace a judikatura doplÅˆujÃ­
- BuÄte struÄnÃ½ a jasnÃ½

NEOPISUJTE celÃ© odpovÄ›di, pouze shrÅˆte hlavnÃ­ zÃ¡vÄ›ry."""

        stream = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[
                {"role": "system", "content": summary_prompt},
                {
                    "role": "user",
                    "content": f"""OTÃZKA:
{question}

WEBOVÃ ODPOVÄšÄ:
{web_answer[:1000]}

ODPOVÄšÄ ZE SOUDNÃCH ROZHODNUTÃ:
{case_answer[:1000]}

VytvoÅ™te krÃ¡tkÃ© shrnutÃ­ (2-3 vÄ›ty):"""
                }
            ],
            temperature=0.3,
            max_tokens=300,
            stream=True,
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    except Exception as e:
        print(f"Error generating summary: {str(e)}")
        yield ""
