from openai import OpenAI

from app.config import settings
from app.models import CaseResult
from app.utils.formatters import format_cases_for_context

SYSTEM_PROMPT = """Jste prÃ¡vnÃ­ analytik specializujÃ­cÃ­ se na ÄeskÃ© prÃ¡vo. VaÅ¡Ã­m Ãºkolem je DETAILNÄš analyzovat poskytnutÃ¡ soudnÃ­ rozhodnutÃ­ a odpovÄ›dÄ›t na otÃ¡zku uÅ¾ivatele s KONKRÃ‰TNÃMI ZÃVÄšRY.

KRITICKÃ PRAVIDLA - ABSOLUTNÃ ZÃKAZ HALUCINACÃ:
1. PouÅ¾Ã­vejte POUZE informace z poskytnutÃ½ch rozhodnutÃ­
2. NIKDY nevymÃ½Å¡lejte prÃ¡vnÃ­ zÃ¡vÄ›ry, kterÃ© nejsou v rozhodnutÃ­ch
3. Pokud rozhodnutÃ­ neobsahujÃ­ odpovÄ›Ä, JASNÄš to Å™eknÄ›te
4. NIKDY neodkazujte na zÃ¡kony nebo paragrafy, kterÃ© nejsou zmÃ­nÄ›ny v rozhodnutÃ­ch
5. Citujte POUZE skuteÄnÃ© ÄÃ¡sti z poskytnutÃ½ch rozhodnutÃ­
6. **NEJDÅ®LEÅ½ITÄšJÅ Ã: Extrahujte KONKRÃ‰TNÃ ZÃVÄšRY a SKUTKOVÃ ZJIÅ TÄšNÃ z kaÅ¾dÃ©ho rozhodnutÃ­**

FORMÃT ODPOVÄšDI:

**ShrnutÃ­ relevance:**
Nejprve v 1-2 vÄ›tÃ¡ch Å™eknÄ›te, zda poskytnutÃ¡ rozhodnutÃ­ odpovÃ­dajÃ­ na otÃ¡zku, nebo ne.

**DetailnÃ­ analÃ½za rozhodnutÃ­:**
Pro KAÅ½DÃ‰ relevantnÃ­ rozhodnutÃ­ uveÄte:

ğŸ“‹ **[SpisovÃ¡ znaÄka]** - [Soud], [Datum]

**SkutkovÃ½ stav:**
[Co se v pÅ™Ã­padÄ› stalo? JakÃ¡ byla situace ÃºÄastnÃ­kÅ¯?]

**KonkrÃ©tnÃ­ prÃ¡vnÃ­ zÃ¡vÄ›ry soudu:**
[Co PÅ˜ESNÄš soud rozhodl? JakÃ© KONKRÃ‰TNÃ zÃ¡vÄ›ry uÄinil?]
- Citujte DOSLOVNÄš klÃ­ÄovÃ© pasÃ¡Å¾e z odÅ¯vodnÄ›nÃ­
- UveÄte KONKRÃ‰TNÃ podmÃ­nky, poÅ¾adavky, nebo kritÃ©ria, kterÃ© soud stanovil

**PouÅ¾itÃ© prÃ¡vnÃ­ pÅ™edpisy:**
[Pouze ty paragrafy, kterÃ© jsou EXPLICITNÄš zmÃ­nÄ›ny v rozhodnutÃ­]

**PÅ™Ã­mÃ¡ odpovÄ›Ä na vaÅ¡i otÃ¡zku:**
[Jak KONKRÃ‰TNÄš toto rozhodnutÃ­ odpovÃ­dÃ¡ na poloÅ¾enou otÃ¡zku?]
[Co z tohoto rozhodnutÃ­ PÅ˜ÃMO vyplÃ½vÃ¡ pro vaÅ¡i situaci?]

---

**SouhrnnÃ¡ odpovÄ›Ä na otÃ¡zku:**

Na zÃ¡kladÄ› analyzovanÃ½ch rozhodnutÃ­:

1. **[PrvnÃ­ hlavnÃ­ zÃ¡vÄ›r]** - Podle rozhodnutÃ­ [^1], soud konkrÃ©tnÄ› stanovil, Å¾e [PÅ˜ESNÃ CITÃT nebo PARAFRÃZE konkrÃ©tnÃ­ho zÃ¡vÄ›ru].

2. **[DruhÃ½ hlavnÃ­ zÃ¡vÄ›r]** - V pÅ™Ã­padÄ› [^2], soud rozhodl, Å¾e [KONKRÃ‰TNÃ zÃ¡vÄ›r s detaily].

3. **[DalÅ¡Ã­ zÃ¡vÄ›ry...]**

**PraktickÃ© shrnutÃ­:**
[Co z tÄ›chto rozhodnutÃ­ KONKRÃ‰TNÄš vyplÃ½vÃ¡ pro odpovÄ›Ä na otÃ¡zku?]

**Pokud rozhodnutÃ­ neodpovÃ­dajÃ­:**
Pokud poskytnutÃ¡ rozhodnutÃ­ neobsahujÃ­ KONKRÃ‰TNÃ odpovÄ›Ä na otÃ¡zku, napiÅ¡te:
"âš ï¸ PoskytnutÃ¡ rozhodnutÃ­ se zabÃ½vajÃ­ [co Å™eÅ¡Ã­], ale NEOBSAHUJÃ konkrÃ©tnÃ­ informace o [co chybÃ­]. Pro pÅ™esnou odpovÄ›Ä by bylo potÅ™eba nalÃ©zt rozhodnutÃ­, kterÃ¡ se pÅ™Ã­mo zabÃ½vajÃ­ [konkrÃ©tnÃ­ tÃ©ma]."

**CitovanÃ© pÅ™Ã­pady:**
[^1]: [SpisovÃ¡ znaÄka], [Soud], [Datum], ECLI: [ECLI]
[^2]: [SpisovÃ¡ znaÄka], [Soud], [Datum], ECLI: [ECLI]

PÅ˜ÃKLAD DOBRÃ‰ ODPOVÄšDI (KONKRÃ‰TNÃ):
"Podle rozhodnutÃ­ NejvyÅ¡Å¡Ã­ho soudu sp. zn. 25 Cdo 1234/2020 [^1] platÃ­, Å¾e 'manÅ¾elÃ© jsou povinni pÅ™edloÅ¾it soudu dohodu o ÃºpravÄ› pomÄ›rÅ¯ k nezletilÃ½m dÄ›tem, kterÃ¡ musÃ­ obsahovat Ãºpravu vÃ½Å¾ivnÃ©ho, bydlenÃ­ a vÃ½chovy dÄ›tÃ­.' Soud v tomto pÅ™Ã­padÄ› konkrÃ©tnÄ› uvedl, Å¾e bez takovÃ© dohody nelze rozvod vyslovit. Toto bylo potvrzeno i v pÅ™Ã­padÄ› [^2], kde soud odmÃ­tl nÃ¡vrh na rozvod, protoÅ¾e manÅ¾elÃ© nepÅ™edloÅ¾ili Ãºplnou dohodu o vÃ½Å¾ivnÃ©m."

PÅ˜ÃKLAD Å PATNÃ‰ ODPOVÄšDI (PÅ˜ÃLIÅ  OBECNÃ‰):
"RozhodnutÃ­ se zabÃ½vÃ¡ rodiÄovskou odpovÄ›dnostÃ­." âŒ (ChybÃ­ konkrÃ©tnÃ­ zÃ¡vÄ›ry!)
"Soud Å™eÅ¡il vÃ½Å¾ivnÃ©." âŒ (Co KONKRÃ‰TNÄš o vÃ½Å¾ivnÃ©m rozhodl?)
"RelevantnÃ­ pro vaÅ¡i otÃ¡zku." âŒ (JAK konkrÃ©tnÄ› je relevantnÃ­?)

PAMATUJTE: 
- BuÄte KONKRÃ‰TNÃ, ne obecnÃ­
- Citujte PÅ˜ESNÃ‰ zÃ¡vÄ›ry, ne jen tÃ©mata
- Extrahujte SKUTEÄŒNÃ ZJIÅ TÄšNÃ, ne jen to, Äeho se pÅ™Ã­pad tÃ½kal
- Pokud v rozhodnutÃ­ nenÃ­ dostatek detailÅ¯, Å˜EKNÄšTE TO"""

SONAR_PROMPT = """Jste prÃ¡vnÃ­ expert se specialistem na ÄeskÃ© prÃ¡vo. OdpovÃ­dejte na otÃ¡zky uÅ¾ivatele VÃHRADNÄš na zÃ¡kladÄ› poskytnutÃ½ch rozhodnutÃ­ ÄeskÃ½ch soudÅ¯.

VaÅ¡e odpovÄ›Ä musÃ­ obsahovat:
1. PÅ™Ã­mou odpovÄ›Ä na poloÅ¾enou otÃ¡zku na zÃ¡kladÄ› pÅ™Ã­sluÅ¡nÃ½ch rozhodnutÃ­
2. Citace vÅ¡ech relevantnÃ­ch, aktuÃ¡lnÃ­ch a konkrÃ©tnÃ­ch zÃ¡konÅ¯, vyhlÃ¡Å¡ek, prÃ¡vnÃ­ch pÅ™edpisÅ¯, prÃ¡vnÃ­ch principÅ¯, zrÃ¡tka zÃ¡kona, musÃ­ obsahovat:
   - KonkrÃ©tnÃ­ paragraf a ÄÃ­slo zÃ¡konu
   - Datum vydÃ¡nÃ­
   - Datum vydÃ¡nÃ­
   - ECLI reference
   - RelevantnÃ­ prÃ¡vnÃ­ pÅ™edpisy (Â§ citace)

OdpovÄ›Ä musÃ­ bÃ½t:
- StrukturovanÃ¡ a logickÃ¡
- PsanÃ¡ v ÄeÅ¡tinÄ›
- SoustÅ™edÄ›na vÃ½hradnÄ› na poskytnutÃ© informace
- Bez generalizacÃ­ nebo informacÃ­ mimo zÃ¡kladnu rozhodnutÃ­
- S pÅ™esnÃ½mi citacemi a odkazem
- MusÃ­ vychÃ¡zet z kontextu, musÃ­ brÃ¡t v potaz i prÃ¡vnÃ­ principy, strukturu a hierarchii zÃ¡konÅ¯
- PouÅ¾Ã­vejte pouze Ãºdaje z oficiÃ¡lnÃ­ch vlÃ¡dnÃ­ch nebo renomovanÃ½ch prÃ¡vnÃ­ch webÅ¯ (napÅ™. zakonyprolidi.cz, nsoud.cz, eur-lex.europa.eu)
- VyhÃ½bejte se citacÃ­m z nÃ¡hodnÃ½ch fÃ³r, diskuznÃ­ch skupin nebo uÅ¾ivatelskÃ½ch komentÃ¡Å™Å¯

Pokud je otÃ¡zka nezodpovÄ›ditelnÃ¡ na zÃ¡kladÄ› tÄ›chto dat a tohoto postupu, vÃ½slovnÄ› to uveÄte."""


def get_openai_client() -> OpenAI:
    """Get configured OpenAI client for OpenRouter"""
    return OpenAI(
        api_key=settings.OPENROUTER_API_KEY,
        base_url=settings.OPENROUTER_BASE_URL,
    )


async def get_sonar_answer(question: str) -> tuple[str, list[str]]:
    """
    Get answer from Perplexity Sonar with citations
    Returns: (answer_text, citations_list)
    """
    try:
        client = get_openai_client()

        sonar_response = client.chat.completions.create(
            model="perplexity/sonar",
            messages=[
                {"role": "system", "content": SONAR_PROMPT},
                {"role": "user", "content": question},
            ],
            stream=False,
        )

        sonar_answer = sonar_response.choices[0].message.content or ""

        # Capture citations
        sonar_citations = getattr(sonar_response, "citations", [])
        if not sonar_citations:
            search_results = getattr(sonar_response, "search_results", [])
            sonar_citations = [
                result.get("url", "") for result in search_results if result.get("url")
            ]

        return sonar_answer, sonar_citations

    except Exception as e:
        print(f"Chyba pri ziskani Sonar odpovedi: {str(e)}")
        return "", []


async def get_sonar_answer_stream(question: str):
    """
    Get streaming answer from Perplexity Sonar with citations
    Yields: (chunk_text, final_answer, citations_list)
    
    Note: Perplexity's streaming API doesn't include citations in chunks.
    We need to make a separate non-streaming call to get citations.
    """
    try:
        client = get_openai_client()

        # Start streaming the answer
        stream = client.chat.completions.create(
            model="perplexity/sonar",
            messages=[
                {"role": "system", "content": SONAR_PROMPT},
                {"role": "user", "content": question},
            ],
            stream=True,
        )

        full_answer = ""
        citations = []

        # Stream the content
        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_answer += content
                yield content, None, None

        # After streaming completes, make a non-streaming call to get citations
        # This is necessary because Perplexity's streaming API doesn't include citations
        try:
            citation_response = client.chat.completions.create(
                model="perplexity/sonar",
                messages=[
                    {"role": "system", "content": SONAR_PROMPT},
                    {"role": "user", "content": question},
                ],
                stream=False,
            )
            
            # Extract citations from the response
            citations = getattr(citation_response, "citations", [])
            if not citations:
                # Fallback to search_results if citations not available
                search_results = getattr(citation_response, "search_results", [])
                citations = [
                    result.get("url", "") for result in search_results if result.get("url")
                ]
        except Exception as citation_error:
            print(f"Error fetching citations: {str(citation_error)}")
            citations = []

        # Final yield with complete answer and citations
        yield None, full_answer, citations

    except Exception as e:
        print(f"Chyba pri ziskani Sonar odpovedi: {str(e)}")
        yield None, "", []


async def answer_based_on_cases(
    question: str, cases: list[CaseResult], client: OpenAI
) -> str:
    """
    GPT-4o answers the question based on FULL case data with citations
    NO TRUNCATION - All context is passed to GPT
    """
    try:
        # Format cases with FULL context - NO TRUNCATION
        cases_context = format_cases_for_context(cases)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“¤ PASSING FULL CONTEXT TO GPT")
        print(f"{'='*80}")
        print(f"Number of cases: {len(cases)}")
        print(f"Context length: {len(cases_context)} characters")
        print(f"Context length: {len(cases_context.split())} words")
        print(f"Estimated tokens: ~{len(cases_context) // 4}")
        print(f"{'='*80}\n")

        response = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {
                    "role": "user",
                    "content": f"""OTÃZKA UÅ½IVATELE:
{question}

POSKYTNUTÃ SOUDNÃ ROZHODNUTÃ (KOMPLETNÃ KONTEXT):
{cases_context}

ÃšKOL - KRITICKY DÅ®LEÅ½ITÃ‰:
1. Pro KAÅ½DÃ‰ rozhodnutÃ­ extrahujte:
   - SkutkovÃ½ stav (co se stalo)
   - KONKRÃ‰TNÃ prÃ¡vnÃ­ zÃ¡vÄ›ry soudu (ne jen tÃ©mata!)
   - PÅ˜ESNÃ‰ citace z odÅ¯vodnÄ›nÃ­
   - Jak KONKRÃ‰TNÄš odpovÃ­dÃ¡ na otÃ¡zku

2. NEÅ˜ÃKEJTE jen "rozhodnutÃ­ se zabÃ½vÃ¡ X" - Å˜EKNÄšTE "soud konkrÃ©tnÄ› rozhodl, Å¾e..."

3. Extrahujte SPECIFICKÃ‰ poÅ¾adavky, podmÃ­nky, kritÃ©ria, kterÃ© soud stanovil

4. Pokud rozhodnutÃ­ neobsahuje KONKRÃ‰TNÃ odpovÄ›Ä na otÃ¡zku, JASNÄš to Å™eknÄ›te

5. NIKDY nevymÃ½Å¡lejte - pokud v rozhodnutÃ­ nenÃ­ dostatek detailÅ¯, pÅ™iznejte to

PÅ˜ÃKLAD Å PATNÃ‰ ANALÃZY:
"RozhodnutÃ­ se zabÃ½vÃ¡ vÃ½Å¾ivnÃ½m." âŒ

PÅ˜ÃKLAD DOBRÃ‰ ANALÃZY:
"Soud v tomto rozhodnutÃ­ stanovil, Å¾e dohoda o vÃ½Å¾ivnÃ©m musÃ­ obsahovat konkrÃ©tnÃ­ ÄÃ¡stku, periodicitu plateb a zpÅ¯sob valorizace. Bez tÄ›chto nÃ¡leÅ¾itostÃ­ soud dohodu neschvÃ¡lil." âœ…

DÅ®LEÅ½ITÃ‰: MÃ¡te k dispozici PLNÃ kontext vÅ¡ech rozhodnutÃ­ bez zkrÃ¡cenÃ­.
ZaÄnÄ›te DETAILNÃ analÃ½zou kaÅ¾dÃ©ho rozhodnutÃ­.""",
                },
            ],
            temperature=0.3,  # Hardcoded: Low temperature to reduce hallucinations
            max_tokens=4000,  # Hardcoded: High limit for detailed responses with full context
        )

        answer = (response.choices[0].message.content or "").strip()
        
        print(f"âœ… GPT response generated: {len(answer)} characters\n")
        
        return answer

    except Exception as e:
        print(f"âŒ Chyba pri generovani odpovedi zalozene na pripadech: {str(e)}")
        import traceback
        traceback.print_exc()
        return ""


async def answer_based_on_cases_stream(
    question: str, cases: list[CaseResult], client: OpenAI
):
    """
    Stream GPT-4o answer based on FULL case data - NO TRUNCATION
    """
    try:
        print(f"\n{'='*80}")
        print(f"ğŸ“¤ STREAMING FULL CONTEXT TO GPT")
        print(f"{'='*80}")
        print(f"Number of cases: {len(cases)}")
        
        # Format cases with FULL context - NO TRUNCATION
        cases_context = format_cases_for_context(cases)
        
        print(f"Context length: {len(cases_context)} characters")
        print(f"Context length: {len(cases_context.split())} words")
        print(f"Estimated tokens: ~{len(cases_context) // 4}")
        print(f"{'='*80}\n")

        print(f"ğŸ¤– Starting OpenAI streaming...")
        stream = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {
                    "role": "user",
                    "content": f"""OTÃZKA UÅ½IVATELE:
{question}

POSKYTNUTÃ SOUDNÃ ROZHODNUTÃ (KOMPLETNÃ KONTEXT):
{cases_context}

ÃšKOL - KRITICKY DÅ®LEÅ½ITÃ‰:
1. Pro KAÅ½DÃ‰ rozhodnutÃ­ extrahujte:
   - SkutkovÃ½ stav (co se stalo)
   - KONKRÃ‰TNÃ prÃ¡vnÃ­ zÃ¡vÄ›ry soudu (ne jen tÃ©mata!)
   - PÅ˜ESNÃ‰ citace z odÅ¯vodnÄ›nÃ­
   - Jak KONKRÃ‰TNÄš odpovÃ­dÃ¡ na otÃ¡zku

2. NEÅ˜ÃKEJTE jen "rozhodnutÃ­ se zabÃ½vÃ¡ X" - Å˜EKNÄšTE "soud konkrÃ©tnÄ› rozhodl, Å¾e..."

3. Extrahujte SPECIFICKÃ‰ poÅ¾adavky, podmÃ­nky, kritÃ©ria, kterÃ© soud stanovil

4. Pokud rozhodnutÃ­ neobsahuje KONKRÃ‰TNÃ odpovÄ›Ä na otÃ¡zku, JASNÄš to Å™eknÄ›te

5. NIKDY nevymÃ½Å¡lejte - pokud v rozhodnutÃ­ nenÃ­ dostatek detailÅ¯, pÅ™iznejte to

PÅ˜ÃKLAD Å PATNÃ‰ ANALÃZY:
"RozhodnutÃ­ se zabÃ½vÃ¡ vÃ½Å¾ivnÃ½m." âŒ

PÅ˜ÃKLAD DOBRÃ‰ ANALÃZY:
"Soud v tomto rozhodnutÃ­ stanovil, Å¾e dohoda o vÃ½Å¾ivnÃ©m musÃ­ obsahovat konkrÃ©tnÃ­ ÄÃ¡stku, periodicitu plateb a zpÅ¯sob valorizace. Bez tÄ›chto nÃ¡leÅ¾itostÃ­ soud dohodu neschvÃ¡lil." âœ…

DÅ®LEÅ½ITÃ‰: MÃ¡te k dispozici PLNÃ kontext vÅ¡ech rozhodnutÃ­ bez zkrÃ¡cenÃ­.
ZaÄnÄ›te DETAILNÃ analÃ½zou kaÅ¾dÃ©ho rozhodnutÃ­.""",
                },
            ],
            temperature=0.3,  # Hardcoded: Low temperature to reduce hallucinations
            max_tokens=4000,  # Hardcoded: High limit for detailed responses with full context
            stream=True,
        )

        chunk_count = 0
        for chunk in stream:
            if chunk.choices[0].delta.content:
                chunk_count += 1
                content = chunk.choices[0].delta.content
                yield content
        
        print(f"âœ… Yielded {chunk_count} chunks from OpenAI")
        
        if chunk_count == 0:
            print("âš ï¸ WARNING: OpenAI returned 0 chunks!")

    except Exception as e:
        print(f"âŒ Chyba pri streamovani odpovedi: {str(e)}")
        import traceback
        traceback.print_exc()



async def generate_combined_summary_stream(
    question: str,
    web_answer: str,
    case_answer: str,
    client: OpenAI
):
    """
    Generate a concise summary combining web and case search results
    """
    try:
        summary_prompt = """Jste prÃ¡vnÃ­ expert. MÃ¡te k dispozici dvÄ› odpovÄ›di na stejnou otÃ¡zku:
1. OdpovÄ›Ä z webovÃ©ho vyhledÃ¡vÃ¡nÃ­ (aktuÃ¡lnÃ­ prÃ¡vnÃ­ informace)
2. OdpovÄ›Ä zaloÅ¾enÃ¡ na soudnÃ­ch rozhodnutÃ­ch (judikatura)

VytvoÅ™te KRÃTKÃ‰ shrnutÃ­ (2-3 vÄ›ty), kterÃ©:
- Syntetizuje obÄ› odpovÄ›di
- ZdÅ¯raznÃ­ klÃ­ÄovÃ© body
- UkÃ¡Å¾e, jak se webovÃ© informace a judikatura doplÅˆujÃ­
- BuÄte struÄnÃ½ a jasnÃ½

NEOPISUJTE celÃ© odpovÄ›di, pouze shrÅˆte hlavnÃ­ zÃ¡vÄ›ry."""

        stream = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[
                {"role": "system", "content": summary_prompt},
                {
                    "role": "user",
                    "content": f"""OTÃZKA:
{question}

WEBOVÃ ODPOVÄšÄ:
{web_answer[:1000]}

ODPOVÄšÄ ZE SOUDNÃCH ROZHODNUTÃ:
{case_answer[:1000]}

VytvoÅ™te krÃ¡tkÃ© shrnutÃ­ (2-3 vÄ›ty):"""
                }
            ],
            temperature=0.3,
            max_tokens=300,
            stream=True,
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    except Exception as e:
        print(f"Error generating summary: {str(e)}")
        yield ""
