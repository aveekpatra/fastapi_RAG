"""
LLM Service - GPT-5-mini optimized
Handles 400K context window, extended thinking, and quality generation
"""
import asyncio
from typing import AsyncIterator, Optional

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)
from langchain_openai import ChatOpenAI

from app.config import settings
from app.models import CaseResult
from app.utils.formatters import format_cases_for_context

# Optimized prompts for GPT-5-mini (detailed legal analysis with reasoning)
SYSTEM_PROMPT = """Jste senior prÃ¡vnÃ­ analytik se specializacÃ­ na ÄeskÃ© prÃ¡vo. VaÅ¡Ã­m Ãºkolem je poskytnout DETAILNÃ, ZDÅ®VODNÄšNOU a PRAKTICKY UÅ½ITEÄŒNOU odpovÄ›Ä na prÃ¡vnÃ­ dotaz klienta.

PRINCIP ANALÃZY:
Nejste pouhÃ½ vyhledÃ¡vaÄ - jste prÃ¡vnÃ­ poradce. VaÅ¡e odpovÄ›Ä musÃ­:
1. PÅ™Ã­mo odpovÄ›dÄ›t na otÃ¡zku (jasnÃ¡ odpovÄ›Ä hned na zaÄÃ¡tku)
2. VysvÄ›tlit LOGIKU a ZDÅ®VODNÄšNÃ (proÄ je to tak, jakÃ© jsou prÃ¡vnÃ­ principy)
3. Citovat PÅ˜ESNÃ‰ pasÃ¡Å¾e z rozhodnutÃ­ s vysvÄ›tlenÃ­m jejich PRAKTICKÃ‰HO VÃZNAMU
4. Identifikovat KLÃÄŒOVÃ‰ PRÃVNÃ POJMY a jejich definice
5. Upozornit na PRAKTICKÃ‰ DÅ®SLEDKY a RIZIKA
6. Porovnat PODOBNÃ‰ SITUACE z judikatury

STRUKTURA ODPOVÄšDI:
1. **PÅ˜ÃMÃ ODPOVÄšÄ** (1-2 vÄ›ty, jasnÄ› a srozumitelnÄ›)
2. **PRÃVNÃ ANALÃZA** (3-5 odstavcÅ¯):
   - VysvÄ›tlete prÃ¡vnÃ­ princip/normu
   - Citujte relevantnÃ­ pasÃ¡Å¾e: > â€pÅ™esnÃ¡ citace" [ÄÃ­slo]
   - VysvÄ›tlete, CO CITACE ZNAMENÃ a PROÄŒ JE DÅ®LEÅ½ITÃ
   - UveÄte PRAKTICKÃ DOPAD na situaci klienta
3. **KLÃÄŒOVÃ‰ BODY** (seznam 3-5 nejdÅ¯leÅ¾itÄ›jÅ¡Ã­ch zjiÅ¡tÄ›nÃ­)
4. **PRAKTICKÃ‰ DOPORUÄŒENÃ** (co by mÄ›l klient dÄ›lat)
5. **PODOBNÃ‰ PÅ˜ÃPADY** (seznam citovanÃ½ch rozhodnutÃ­ s jejich tÃ©maty)

FORMÃT CITACÃ - VELMI DÅ®LEÅ½ITÃ‰:
> â€pÅ™esnÃ¡ citace z rozhodnutÃ­" [ÄÃ­slo]

PÅ˜ÃKLAD DOBRÃ‰ ANALÃZY:
OtÃ¡zka: MÃ¡ prÃ¡vnickÃ¡ osoba prÃ¡vo na nÃ¡hradu nemajetkovÃ© Ãºjmy?

OdpovÄ›Ä: Ano, prÃ¡vnickÃ¡ osoba mÃ¡ prÃ¡vo na nÃ¡hradu nemajetkovÃ© Ãºjmy pÅ™i poruÅ¡enÃ­ jejÃ­ osobnostnÃ­ch prÃ¡v.

ZdÅ¯vodnÄ›nÃ­: NejvyÅ¡Å¡Ã­ soud v rozhodnutÃ­ [1] jasnÄ› stanovil, Å¾e > â€prÃ¡vnickÃ¡ osoba mÃ¡ prÃ¡vo na nÃ¡hradu nemajetkovÃ© Ãºjmy pÅ™i zÃ¡sahu do jejÃ­ dobrÃ© povÄ›sti" [1]. To znamenÃ¡, Å¾e pokud dojde k poruÅ¡enÃ­ reputace nebo dÅ¯vÄ›ryhodnosti spoleÄnosti, mÃ¡ prÃ¡vo na kompenzaci. Soud v [2] dÃ¡le upÅ™esnil, Å¾e > â€vÃ½Å¡e nÃ¡hrady se posuzuje s ohledem na zÃ¡vaÅ¾nost poruÅ¡enÃ­ a postavenÃ­ osoby" [2], coÅ¾ znamenÃ¡, Å¾e vÄ›tÅ¡Ã­ spoleÄnosti mohou mÃ­t nÃ¡rok na vyÅ¡Å¡Ã­ nÃ¡hradu.

PraktickÃ½ dopad: Pokud byla vaÅ¡e spoleÄnost veÅ™ejnÄ› znevÃ¡Å¾ena, mÃ¡te prÃ¡vnÃ­ zÃ¡klad pro Å¾alobu na nÃ¡hradu.

KRITICKÃ PRAVIDLA:
âœ“ Citujte DOSLOVNÄš, ne parafrÃ¡zÃ­
âœ“ VysvÄ›tlete LOGIKU za kaÅ¾dou citacÃ­
âœ“ BuÄte PRAKTIÄŒTÃ - Å™eknÄ›te, co to znamenÃ¡ pro klienta
âœ“ Identifikujte RIZIKA a NEJISTOTY
âœ“ ZmÃ­Åˆujte VÃJIMKY a OMEZENÃ
âœ— NezmiÅˆujte nerelevantnÃ­ rozhodnutÃ­
âœ— NebuÄte pÅ™Ã­liÅ¡ kreativnÃ­ - drÅ¾te se faktÅ¯
âœ— NepÅ™edpoklÃ¡dejte prÃ¡vnÃ­ znalosti klienta - vysvÄ›tlujte pojmy

TÃ“NUS:
- ProfesionÃ¡lnÃ­, ale srozumitelnÃ½
- DetailnÃ­, ale struÄnÃ½ (ne vÃ­ce neÅ¾ 1000 slov)
- SebevÄ›domÃ½ v prÃ¡vnÃ­ch otÃ¡zkÃ¡ch, ale opatrnÃ½ v pÅ™edpovÄ›dÃ­ch
- Prakticky zamÄ›Å™enÃ½ na Å™eÅ¡enÃ­ problÃ©mu

Pokud Å½ÃDNÃ‰ rozhodnutÃ­ neodpovÃ­dÃ¡: "âš ï¸ Å½ÃDNÃ‰ RELEVANTNÃ PÅ˜ÃPADY - VaÅ¡e situace nenÃ­ v dostupnÃ© judikatuÅ™e Å™eÅ¡ena. DoporuÄuji konzultaci s prÃ¡vnÃ­kem."
"""

SONAR_PROMPT = """Jste prÃ¡vnÃ­ expert na ÄeskÃ© prÃ¡vo a LEGISLATIVU. OdpovÃ­dejte na zÃ¡kladÄ› AKTUÃLNÃCH ZÃKONÅ®.

Citujte konkrÃ©tnÃ­ paragrafy (napÅ™. Â§ 123 zÃ¡kona Ä. 89/2012 Sb.) s odkazy na zakonyprolidi.cz.
VYHÃBEJTE SE citacÃ­m soudnÃ­ch rozhodnutÃ­."""

QUERY_GENERATION_PROMPT = """Vygenerujte 3-4 optimalizovanÃ© vyhledÃ¡vacÃ­ dotazy pro prÃ¡vnÃ­ databÃ¡zi ÄeskÃ½ch soudnÃ­ch rozhodnutÃ­.

STRATEGIE:
1. PÅ˜ÃMÃ DOTAZ - PÅ™eformulujte otÃ¡zku s prÃ¡vnÃ­ terminologiÃ­
2. KLÃÄŒOVÃ‰ POJMY - Vyhledejte hlavnÃ­ prÃ¡vnÃ­ koncepty
3. SYNONYMA - PouÅ¾ijte prÃ¡vnÃ­ synonyma a alternativnÃ­ formulace
4. SPECIFIKA - ZamÄ›Å™te se na konkrÃ©tnÃ­ aspekty problÃ©mu

PRAVIDLA:
- Vygenerujte POUZE dotazy, Å½ÃDNÃ dalÅ¡Ã­ text
- Max 12 slov na dotaz
- PouÅ¾Ã­vejte prÃ¡vnÃ­ terminologii (napÅ™. "nÃ¡hrada Å¡kody", "poruÅ¡enÃ­ smlouvy")
- RÅ¯znÃ© Ãºhly pohledu na stejnÃ½ problÃ©m
- Jeden dotaz na Å™Ã¡dek, bez ÄÃ­slovÃ¡nÃ­
- POUZE ÄistÃ© vyhledÃ¡vacÃ­ dotazy

PÅ˜ÃKLADY DOBRÃCH DOTAZÅ®:
- "prÃ¡vo na nÃ¡hradu nemajetkovÃ© Ãºjmy prÃ¡vnickÃ© osoby"
- "poruÅ¡enÃ­ dobrÃ© povÄ›sti spoleÄnosti odÅ¡kodnÄ›nÃ­"
- "nemajetkovÃ¡ Ãºjma prÃ¡vnickÃ© osoby judikatura"

OTÃZKA: {question}

DOTAZY:"""

RERANK_PROMPT = """SeÅ™aÄte rozhodnutÃ­ podle relevance a uÅ¾iteÄnosti pro prÃ¡vnÃ­ analÃ½zu dotazu.

KRITÃ‰RIA RELEVANCE (v poÅ™adÃ­ dÅ¯leÅ¾itosti):
1. PÅ˜ÃMÃ RELEVANCE - RozhodnutÃ­ pÅ™Ã­mo Å™eÅ¡Ã­ stejnÃ½ prÃ¡vnÃ­ problÃ©m
2. PRÃVNÃ PRINCIP - RozhodnutÃ­ stanovuje klÃ­ÄovÃ½ prÃ¡vnÃ­ princip aplikovatelnÃ½ na dotaz
3. PRAKTICKÃ UÅ½ITEÄŒNOST - RozhodnutÃ­ poskytuje praktickÃ© vodÃ­tko pro Å™eÅ¡enÃ­
4. AKTUÃLNOST - NovÄ›jÅ¡Ã­ rozhodnutÃ­ jsou preferovÃ¡na (pokud nejsou zruÅ¡ena)
5. AUTORITA - RozhodnutÃ­ NejvyÅ¡Å¡Ã­ho soudu > ÃšstavnÃ­ho soudu > ostatnÃ­

IGNORUJTE:
- RozhodnutÃ­, kterÃ¡ Å™eÅ¡Ã­ zcela jinÃ½ prÃ¡vnÃ­ problÃ©m
- RozhodnutÃ­, kterÃ¡ jsou pouze okrajovÄ› relevantnÃ­

DOTAZ: {query}

ROZHODNUTÃ:
{cases}

SEÅ˜AZENÃ‰ INDEXY (napÅ™. "2,0,4,1"):"""



class LLMService:
    """
    GPT-5-mini optimized LLM service
    - 400K token context window
    - Extended thinking support
    - Streaming with reasoning tokens handling
    - Ultra-fast nano model for simple tasks
    """

    def __init__(self):
        self._gpt_model: Optional[ChatOpenAI] = None
        self._sonar_model: Optional[ChatOpenAI] = None
        self._fast_model: Optional[ChatOpenAI] = None
        self._chains = {}

    @property
    def gpt_model(self) -> ChatOpenAI:
        """Main GPT-5-mini model for complex reasoning tasks"""
        if self._gpt_model is None:
            self._gpt_model = ChatOpenAI(
                model=settings.LLM_MODEL,
                api_key=settings.OPENROUTER_API_KEY,
                base_url=settings.OPENROUTER_BASE_URL,
                temperature=settings.LLM_TEMPERATURE,
                max_tokens=settings.LLM_MAX_TOKENS,
                timeout=settings.LLM_TIMEOUT,
                extra_body={
                    "provider": {"order": ["OpenAI"], "allow_fallbacks": True},
                    # GPT-5-mini thinking budget (if supported)
                    "thinking": {"budget_tokens": settings.LLM_THINKING_BUDGET},
                },
            )
        return self._gpt_model

    @property
    def fast_model(self) -> ChatOpenAI:
        """GPT-5-nano for ultra-fast simple tasks (query gen, reranking)"""
        if self._fast_model is None:
            self._fast_model = ChatOpenAI(
                model=settings.FAST_MODEL,
                api_key=settings.OPENROUTER_API_KEY,
                base_url=settings.OPENROUTER_BASE_URL,
                temperature=0.3,
                max_tokens=2000,
                timeout=60.0,
                extra_body={
                    "provider": {"order": ["OpenAI", "Azure"], "allow_fallbacks": True}
                },
            )
        return self._fast_model

    @property
    def sonar_model(self) -> ChatOpenAI:
        """Perplexity Sonar for web search"""
        if self._sonar_model is None:
            self._sonar_model = ChatOpenAI(
                model="perplexity/sonar",
                api_key=settings.OPENROUTER_API_KEY,
                base_url=settings.OPENROUTER_BASE_URL,
                temperature=0.7,
                timeout=settings.LLM_TIMEOUT,
            )
        return self._sonar_model

    def _get_case_answer_chain(self):
        if "case_answer" not in self._chains:
            prompt = ChatPromptTemplate.from_messages([
                SystemMessagePromptTemplate.from_template(SYSTEM_PROMPT),
                HumanMessagePromptTemplate.from_template(
                    """OTÃZKA KLIENTA: {question}

DOSTUPNÃ ROZHODNUTÃ:
{context}

INSTRUKCE:
1. PÅ™eÄtÄ›te si otÃ¡zku
2. NajdÄ›te POUZE rozhodnutÃ­, kterÃ¡ pÅ™Ã­mo odpovÃ­dajÃ­ na otÃ¡zku
3. Ignorujte vÅ¡echna ostatnÃ­ rozhodnutÃ­
4. OdpovÄ›zte struÄnÄ› s citacemi [^1], [^2] pouze z relevantnÃ­ch rozhodnutÃ­
5. NEZMIÅ‡UJTE rozhodnutÃ­, kterÃ¡ nejsou relevantnÃ­

ODPOVÄšÄ:"""
                ),
            ])
            self._chains["case_answer"] = prompt | self.gpt_model | StrOutputParser()
        return self._chains["case_answer"]

    def _get_query_chain(self):
        if "query" not in self._chains:
            prompt = ChatPromptTemplate.from_messages([
                HumanMessagePromptTemplate.from_template(QUERY_GENERATION_PROMPT),
            ])
            self._chains["query"] = prompt | self.fast_model | StrOutputParser()
        return self._chains["query"]

    async def generate_search_queries(self, question: str, num_queries: int = 5) -> list[str]:
        """Generate optimized search queries using GPT-5-nano (ultra-fast)
        
        Dynamically generates 3-5 queries based on question complexity.
        Always includes the original question.
        """
        try:
            print(f"ğŸ” Query generation input:")
            print(f"   Question type: {type(question)}")
            print(f"   Question length: {len(question)} chars")
            print(f"   Question: {question[:200]}{'...' if len(question) > 200 else ''}")
            
            chain = self._get_query_chain()
            result = await chain.ainvoke({"question": question})

            queries = [
                q.strip()
                for q in result.split("\n")
                if q.strip() and not q.strip().startswith(("1.", "2.", "3.", "4.", "5.", "-", "*", "â€¢"))
            ]
            
            # Validate - allow longer queries for complex legal terms
            validated = [q for q in queries if 2 <= len(q.split()) <= 15]
            
            # Always include original question first
            final = [question]
            for q in validated:
                if q.lower() != question.lower() and len(final) < num_queries:
                    final.append(q)

            print(f"âœ… Generated {len(final)} queries (GPT-5-nano)")
            for i, q in enumerate(final):
                print(f"Query: {q}")
            return final
        except Exception as e:
            print(f"âŒ Query generation error: {e}")
            return [question]

    async def answer_based_on_cases(self, question: str, cases: list[CaseResult]) -> str:
        """Generate answer with GPT-5-mini - handles 400K context"""
        try:
            context = format_cases_for_context(cases)
            
            # GPT-5-mini can handle massive context efficiently
            context_tokens = len(context) // 4
            print(f"ğŸ“¤ Sending {len(cases)} cases to GPT-5-mini")
            print(f"   Context: {len(context):,} chars (~{context_tokens:,} tokens)")
            
            # Warn if approaching limit (400K)
            if context_tokens > 350000:
                print(f"âš ï¸ Large context - truncating to fit 400K window")
                context = context[:1400000]  # ~350K tokens

            chain = self._get_case_answer_chain()
            
            # GPT-5-mini is faster than 4.1-mini but may still think
            answer = await asyncio.wait_for(
                chain.ainvoke({"question": question, "context": context}),
                timeout=settings.LLM_TIMEOUT
            )

            print(f"âœ… Response: {len(answer):,} chars")
            return answer
        except asyncio.TimeoutError:
            print("â±ï¸ GPT-5-mini timeout")
            return "âš ï¸ ÄŒasovÃ½ limit vyprÅ¡el. Zkuste kratÅ¡Ã­ dotaz."
        except Exception as e:
            print(f"âŒ Answer error: {e}")
            return ""

    async def answer_based_on_cases_stream(
        self, question: str, cases: list[CaseResult]
    ) -> AsyncIterator[str]:
        """Stream answer - handles GPT-5-mini thinking tokens"""
        try:
            context = format_cases_for_context(cases)
            print(f"ğŸ“¤ Streaming {len(cases)} cases (GPT-5-mini)")
            print(f"ğŸ“ Question being sent to LLM: {question[:300]}...")
            print(f"ğŸ“Š Context length: {len(context):,} chars")
            
            # Debug: Show first case info
            if cases:
                print(f"ğŸ“‹ First case: {cases[0].case_number} - {(cases[0].subject or '')[:100]}...")

            chain = self._get_case_answer_chain()
            
            chunk_count = 0
            thinking_skipped = 0
            full_answer = ""
            
            async for chunk in chain.astream({"question": question, "context": context}):
                # GPT-5-mini may emit thinking/reasoning tokens - filter them
                if chunk:
                    # Skip thinking markers and internal reasoning
                    if any(marker in chunk for marker in ["<think>", "</think>", "<reasoning>", "</reasoning>"]):
                        thinking_skipped += 1
                        continue
                    chunk_count += 1
                    full_answer += chunk
                    yield chunk
            
            if thinking_skipped:
                print(f"âœ… Streamed {chunk_count} chunks (filtered {thinking_skipped} thinking tokens)")
            else:
                print(f"âœ… Streamed {chunk_count} chunks")
            
            # Debug: Show answer summary
            print(f"ğŸ“ Answer preview: {full_answer[:500]}...")
            if "âš ï¸ Å½ÃDNÃ‰ RELEVANTNÃ PÅ˜ÃPADY" in full_answer:
                print(f"âš ï¸ LLM returned 'no relevant cases' despite having {len(cases)} cases!")
            
        except Exception as e:
            print(f"âŒ Streaming error: {e}")

    async def get_sonar_answer(self, question: str) -> tuple[str, list[str]]:
        """Get web answer from Perplexity Sonar"""
        try:
            messages = [
                SystemMessage(content=SONAR_PROMPT),
                HumanMessage(content=question)
            ]
            response = await self.sonar_model.ainvoke(messages)

            citations = []
            if hasattr(response, "response_metadata"):
                citations = response.response_metadata.get("citations", [])

            return response.content or "", citations
        except Exception as e:
            print(f"âŒ Sonar error: {e}")
            return "", []

    async def get_sonar_answer_stream(self, question: str):
        """Stream Sonar answer"""
        try:
            messages = [
                SystemMessage(content=SONAR_PROMPT),
                HumanMessage(content=question)
            ]
            full_answer = ""

            async for chunk in self.sonar_model.astream(messages):
                if chunk.content:
                    full_answer += chunk.content
                    yield chunk.content, None, None

            # Get citations
            try:
                response = await self.sonar_model.ainvoke(messages)
                citations = response.response_metadata.get("citations", []) if hasattr(response, "response_metadata") else []
            except Exception:
                citations = []

            yield None, full_answer, citations
        except Exception as e:
            print(f"âŒ Sonar stream error: {e}")
            yield None, "", []

    async def generate_summary_stream(
        self, question: str, web_answer: str, case_answer: str
    ) -> AsyncIterator[str]:
        """Generate summary combining web and case answers (uses fast model)"""
        try:
            prompt = f"""ShrÅˆte v 2-3 vÄ›tÃ¡ch hlavnÃ­ zÃ¡vÄ›ry:

OTÃZKA: {question}
WEB: {web_answer[:3000]}
JUDIKATURA: {case_answer[:3000]}

SHRNUTÃ:"""

            async for chunk in self.fast_model.astream(prompt):
                if chunk.content:
                    yield chunk.content
        except Exception as e:
            print(f"âŒ Summary error: {e}")

    async def rerank_cases(self, query: str, cases: list[CaseResult]) -> list[CaseResult]:
        """
        Rerank cases using GPT-5-nano for speed
        Returns reordered list by relevance
        """
        if len(cases) <= 3:
            return cases
        
        try:
            # Build case summaries
            case_summaries = []
            for i, case in enumerate(cases):
                summary = f"[{i}] {case.case_number}: {(case.subject or '')[:200]}"
                case_summaries.append(summary)
            
            prompt = RERANK_PROMPT.format(
                query=query,
                cases="\n".join(case_summaries)
            )
            
            # Use fast model for reranking
            response = await self.fast_model.ainvoke(prompt)
            
            # Parse indices
            indices_str = response.content.strip()
            indices = [int(i.strip()) for i in indices_str.split(",") if i.strip().isdigit()]
            
            # Reorder
            reranked = []
            for idx in indices:
                if 0 <= idx < len(cases):
                    reranked.append(cases[idx])
            
            # Add missing
            for case in cases:
                if case not in reranked:
                    reranked.append(case)
            
            print(f"ğŸ”„ Reranked {len(cases)} cases (GPT-5-nano)")
            return reranked
            
        except Exception as e:
            print(f"âš ï¸ Reranking failed: {e}")
            return cases


# Global instance
llm_service = LLMService()
