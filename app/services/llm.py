"""
LLM Service - Optimized for Quality
Focus: Better queries, better answers
"""
import asyncio
from typing import AsyncIterator, Optional, List

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_openai import ChatOpenAI

from app.config import settings
from app.models import CaseResult


# =============================================================================
# PROMPTS - Optimized for Czech legal search
# =============================================================================

QUERY_PROMPT = """Jsi expert na ƒçesk√© pr√°vo. Vygeneruj 5-7 r≈Øzn√Ωch vyhled√°vac√≠ch dotaz≈Ø pro pr√°vn√≠ datab√°zi.

STRATEGIE:
1. P≈ô√≠m√Ω dotaz - p≈ôesn√° formulace ot√°zky
2. Pr√°vn√≠ terminologie - pou≈æij odborn√© term√≠ny
3. Synonyma - r≈Øzn√© zp≈Øsoby vyj√°d≈ôen√≠
4. Specifick√© aspekty - rozdƒõl na d√≠lƒç√≠ ot√°zky
5. Obecnƒõj≈°√≠ dotaz - ≈°ir≈°√≠ kontext
6. Konkr√©tnƒõj≈°√≠ dotaz - specifick√© detaily

PRAVIDLA:
- Ka≈æd√Ω dotaz na nov√Ω ≈ô√°dek
- Bez ƒç√≠slov√°n√≠
- Max 15 slov na dotaz
- Pou≈æij ƒçeskou pr√°vn√≠ terminologii
- R≈Øzn√© √∫hly pohledu

P≈ò√çKLAD pro "n√°hrada ≈°kody p≈ôi dopravn√≠ nehodƒõ":
n√°hrada ≈°kody dopravn√≠ nehoda
od≈°kodnƒõn√≠ √∫jma na zdrav√≠ autonehoda
bolestn√© zt√≠≈æen√≠ spoleƒçensk√©ho uplatnƒõn√≠
odpovƒõdnost za ≈°kodu provoz vozidla
pojistn√© plnƒõn√≠ povinn√© ruƒçen√≠
regres poji≈°≈•ovny vin√≠k nehody

OT√ÅZKA: {question}

DOTAZY:"""


ANSWER_PROMPT = """Jsi zku≈°en√Ω ƒçesk√Ω pr√°vn√≠ analytik. Odpovƒõz na ot√°zku na z√°kladƒõ soudn√≠ch rozhodnut√≠.

KRITICK√Å PRAVIDLA:
1. Odpovƒõz P≈ò√çMO na ot√°zku - prvn√≠ vƒõta mus√≠ b√Ωt jasn√° odpovƒõƒè
2. Cituj DOSLOVNƒö z rozhodnut√≠: > ‚Äûp≈ôesn√° citace" [ƒç√≠slo]
3. Vysvƒõtli, co citace znamen√° a proƒç je d≈Øle≈æit√°
4. Pokud rozhodnut√≠ NEODPOV√çDAJ√ç na ot√°zku, ≈ôekni: "Nem√°m odpovƒõƒè na tuto ot√°zku."
5. NECITUJ rozhodnut√≠, kter√° nejsou relevantn√≠!

STRUKTURA:
1. **Odpovƒõƒè:** (1-2 vƒõty, jasnƒõ)
2. **Anal√Ωza:** (citace s vysvƒõtlen√≠m)
3. **Z√°vƒõr:** (praktick√© shrnut√≠)

FORM√ÅT CITACE:
> ‚Äûp≈ôesn√° citace z textu rozhodnut√≠" [1]

To znamen√°, ≈æe... (vysvƒõtlen√≠)

OT√ÅZKA: {question}

ROZHODNUT√ç:
{context}

ODPOVƒöƒé:"""


# =============================================================================
# LLM SERVICE
# =============================================================================

class LLMService:
    def __init__(self):
        self._main_model: Optional[ChatOpenAI] = None
        self._fast_model: Optional[ChatOpenAI] = None
    
    @property
    def main_model(self) -> ChatOpenAI:
        if self._main_model is None:
            self._main_model = ChatOpenAI(
                model=settings.LLM_MODEL,
                api_key=settings.OPENROUTER_API_KEY,
                base_url=settings.OPENROUTER_BASE_URL,
                temperature=0.1,  # Lower for more focused answers
                max_tokens=settings.LLM_MAX_TOKENS,
                timeout=settings.LLM_TIMEOUT,
            )
        return self._main_model
    
    @property
    def fast_model(self) -> ChatOpenAI:
        if self._fast_model is None:
            self._fast_model = ChatOpenAI(
                model=settings.FAST_MODEL,
                api_key=settings.OPENROUTER_API_KEY,
                base_url=settings.OPENROUTER_BASE_URL,
                temperature=0.5,  # More creative for query generation
                max_tokens=2000,
                timeout=60.0,
            )
        return self._fast_model
    
    async def generate_search_queries(self, question: str, num_queries: int = 7) -> List[str]:
        """Generate multiple search queries for better recall"""
        try:
            prompt = ChatPromptTemplate.from_messages([
                HumanMessagePromptTemplate.from_template(QUERY_PROMPT)
            ])
            chain = prompt | self.fast_model | StrOutputParser()
            
            result = await chain.ainvoke({"question": question})
            
            # Parse queries - be more lenient
            queries = []
            for line in result.split("\n"):
                line = line.strip()
                # Skip empty lines and lines that look like instructions
                if not line or len(line) < 5:
                    continue
                if line.startswith(("-", "*", "‚Ä¢", "1.", "2.", "3.")):
                    line = line.lstrip("-*‚Ä¢0123456789. ")
                if len(line) >= 5:
                    queries.append(line)
            
            # Always include original question first
            final = [question]
            for q in queries:
                if q.lower() != question.lower() and q not in final:
                    final.append(q)
            
            print(f"‚úÖ Generated {len(final)} queries:")
            for q in final[:5]:
                print(f"   ‚Ä¢ {q[:60]}...")
            
            return final[:num_queries]
            
        except Exception as e:
            print(f"‚ö†Ô∏è Query generation failed: {e}")
            return [question]
    
    def _format_cases_for_context(self, cases: List[CaseResult]) -> str:
        """Format cases for LLM - include ALL available text with clear truncation"""
        parts = []
        total_chars = 0
        max_total_chars = 100000  # ~25k tokens, safe for most models
        max_per_case = 15000  # ~3.7k tokens per case
        
        for i, case in enumerate(cases, 1):
            text = case.subject or ""
            original_length = len(text)
            
            if not text:
                text = "[Text rozhodnut√≠ nen√≠ k dispozici]"
            
            # Truncate if needed with clear marker
            truncated = False
            if len(text) > max_per_case:
                text = text[:max_per_case]
                truncated = True
            
            # Check total context size
            if total_chars + len(text) > max_total_chars:
                remaining = max_total_chars - total_chars
                if remaining > 1000:
                    text = text[:remaining]
                    truncated = True
                else:
                    # Add note that more cases were skipped
                    parts.append(f"\n[Dal≈°√≠ch {len(cases) - i + 1} rozhodnut√≠ vynech√°no kv≈Øli limitu kontextu]")
                    break
            
            truncation_note = ""
            if truncated:
                truncation_note = f"\n[‚ö†Ô∏è Text zkr√°cen z {original_length:,} na {len(text):,} znak≈Ø]"
            
            parts.append(f"""
{'‚ïê'*80}
[{i}] {case.case_number}
Soud: {case.court}
Datum: {case.date_issued or "N/A"}
Relevance sk√≥re: {case.relevance_score:.3f}
D√©lka textu: {len(text):,} znak≈Ø{truncation_note}
{'‚ïê'*80}

{text}
""")
            total_chars += len(text)
        
        result = "\n".join(parts)
        print(f"   üìÑ Context: {len(result):,} chars, {len(cases)} cases")
        return result
    
    async def answer_based_on_cases(self, question: str, cases: List[CaseResult]) -> str:
        """Generate answer - let LLM decide what's relevant"""
        if not cases:
            return "Nem√°m odpovƒõƒè na tuto ot√°zku. V datab√°zi jsem nena≈°el ≈æ√°dn√° soudn√≠ rozhodnut√≠."
        
        try:
            context = self._format_cases_for_context(cases)
            
            print(f"üì§ Sending {len(cases)} cases to LLM")
            print(f"   Context: {len(context):,} chars")
            
            prompt = ChatPromptTemplate.from_messages([
                HumanMessagePromptTemplate.from_template(ANSWER_PROMPT)
            ])
            chain = prompt | self.main_model | StrOutputParser()
            
            answer = await chain.ainvoke({
                "question": question,
                "context": context
            })
            
            return answer
            
        except Exception as e:
            print(f"‚ö†Ô∏è Answer generation failed: {e}")
            return "Do≈°lo k chybƒõ p≈ôi generov√°n√≠ odpovƒõdi."
    
    async def answer_based_on_cases_stream(
        self, question: str, cases: List[CaseResult]
    ) -> AsyncIterator[str]:
        """Stream answer"""
        if not cases:
            yield "Nem√°m odpovƒõƒè na tuto ot√°zku. V datab√°zi jsem nena≈°el ≈æ√°dn√° soudn√≠ rozhodnut√≠."
            return
        
        try:
            context = self._format_cases_for_context(cases)
            
            print(f"üì§ Streaming {len(cases)} cases")
            print(f"   Context: {len(context):,} chars")
            
            prompt = ChatPromptTemplate.from_messages([
                HumanMessagePromptTemplate.from_template(ANSWER_PROMPT)
            ])
            chain = prompt | self.main_model | StrOutputParser()
            
            async for chunk in chain.astream({
                "question": question,
                "context": context
            }):
                if chunk:
                    yield chunk
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Streaming failed: {e}")
            yield "Do≈°lo k chybƒõ p≈ôi generov√°n√≠ odpovƒõdi."
    
    # Skip relevance filtering - cross-encoder handles this now
    async def filter_relevant_cases(
        self, question: str, cases: List[CaseResult], max_cases: int = 10
    ) -> List[CaseResult]:
        """Just return cases - cross-encoder already filtered"""
        return cases[:max_cases]
    
    async def rerank_cases(self, query: str, cases: List[CaseResult]) -> List[CaseResult]:
        """Reranking is now done by cross-encoder in search"""
        return cases
    
    # Sonar for web search
    async def get_sonar_answer(self, question: str) -> tuple[str, list[str]]:
        try:
            sonar = ChatOpenAI(
                model="perplexity/sonar",
                api_key=settings.OPENROUTER_API_KEY,
                base_url=settings.OPENROUTER_BASE_URL,
                temperature=0.7,
                timeout=settings.LLM_TIMEOUT,
            )
            
            response = await sonar.ainvoke([
                SystemMessage(content="Jsi pr√°vn√≠ expert na ƒçesk√© pr√°vo. Odpov√≠dej ƒçesky."),
                HumanMessage(content=question)
            ])
            
            citations = []
            if hasattr(response, "response_metadata"):
                citations = response.response_metadata.get("citations", [])
            
            return response.content or "", citations
            
        except Exception as e:
            print(f"‚ö†Ô∏è Sonar error: {e}")
            return "", []
    
    async def get_sonar_answer_stream(self, question: str):
        try:
            sonar = ChatOpenAI(
                model="perplexity/sonar",
                api_key=settings.OPENROUTER_API_KEY,
                base_url=settings.OPENROUTER_BASE_URL,
                temperature=0.7,
            )
            
            messages = [
                SystemMessage(content="Jsi pr√°vn√≠ expert na ƒçesk√© pr√°vo. Odpov√≠dej ƒçesky."),
                HumanMessage(content=question)
            ]
            
            full_answer = ""
            async for chunk in sonar.astream(messages):
                if chunk.content:
                    full_answer += chunk.content
                    yield chunk.content, None, None
            
            yield None, full_answer, []
            
        except Exception as e:
            print(f"‚ö†Ô∏è Sonar stream error: {e}")
            yield None, "", []
    
    async def generate_summary_stream(
        self, question: str, web_answer: str, case_answer: str
    ) -> AsyncIterator[str]:
        try:
            prompt = f"""Shr≈à hlavn√≠ z√°vƒõry v 2-3 vƒõt√°ch ƒçesky:

OT√ÅZKA: {question}
WEB: {web_answer[:2000]}
JUDIKATURA: {case_answer[:2000]}

SHRNUT√ç:"""
            
            async for chunk in self.fast_model.astream(prompt):
                if chunk.content:
                    yield chunk.content
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Summary error: {e}")


# Global instance
llm_service = LLMService()
