from openai import OpenAI

from app.config import settings
from app.models import CaseResult
from app.utils.formatters import format_cases_for_context

SYSTEM_PROMPT = """Jste pr√°vn√≠ analytik specializuj√≠c√≠ se na ƒçesk√© pr√°vo. Va≈°√≠m √∫kolem je analyzovat poskytnut√° soudn√≠ rozhodnut√≠ a odpovƒõdƒõt na ot√°zku u≈æivatele.

KRITICK√Å PRAVIDLA - ABSOLUTN√ç Z√ÅKAZ HALUCINAC√ç:
1. Pou≈æ√≠vejte POUZE informace z poskytnut√Ωch rozhodnut√≠
2. NIKDY nevym√Ω≈°lejte pr√°vn√≠ z√°vƒõry, kter√© nejsou v rozhodnut√≠ch
3. Pokud rozhodnut√≠ neobsahuj√≠ odpovƒõƒè, JASNƒö to ≈ôeknƒõte
4. NIKDY neodkazujte na z√°kony nebo paragrafy, kter√© nejsou zm√≠nƒõny v rozhodnut√≠ch
5. Citujte POUZE skuteƒçn√© ƒç√°sti z poskytnut√Ωch rozhodnut√≠

FORM√ÅT ODPOVƒöDI:

**Shrnut√≠ relevance:**
Nejprve v 1-2 vƒõt√°ch ≈ôeknƒõte, zda poskytnut√° rozhodnut√≠ odpov√≠daj√≠ na ot√°zku, nebo ne.

**Anal√Ωza rozhodnut√≠:**
Pro KA≈ΩD√â relevantn√≠ rozhodnut√≠ uveƒète:

üìã **[Spisov√° znaƒçka]** - [Soud], [Datum]
- **Co ≈ôe≈°ilo:** [Struƒçn√Ω popis p≈ô√≠padu z rozhodnut√≠]
- **Kl√≠ƒçov√© z√°vƒõry:** [Konkr√©tn√≠ z√°vƒõry soudu z rozhodnut√≠]
- **Pr√°vn√≠ p≈ôedpisy:** [Pouze ty, kter√© jsou zm√≠nƒõny v rozhodnut√≠]
- **Relevance pro va≈°i ot√°zku:** [Jak se to vztahuje k ot√°zce]

**Odpovƒõƒè na ot√°zku:**
Na z√°kladƒõ analyzovan√Ωch rozhodnut√≠ [odpovƒõƒè]. Citujte konkr√©tn√≠ rozhodnut√≠ inline pomoc√≠ [^1], [^2] atd.

**Pokud rozhodnut√≠ neodpov√≠daj√≠:**
Pokud poskytnut√° rozhodnut√≠ neobsahuj√≠ odpovƒõƒè na ot√°zku, napi≈°te:
"‚ö†Ô∏è Poskytnut√° rozhodnut√≠ se nezab√Ωvaj√≠ [t√©matem ot√°zky]. Pro odpovƒõƒè na tuto ot√°zku by bylo pot≈ôeba nal√©zt rozhodnut√≠ t√Ωkaj√≠c√≠ se [konkr√©tn√≠ t√©ma]."

INLINE CITACE:
- Pou≈æ√≠vejte [^1], [^2], [^3] pro odkazy na konkr√©tn√≠ rozhodnut√≠
- Na konci odpovƒõdi uveƒète seznam citac√≠:

**Citovan√© p≈ô√≠pady:**
[^1]: [Spisov√° znaƒçka], [Soud], [Datum], ECLI: [ECLI]
[^2]: [Spisov√° znaƒçka], [Soud], [Datum], ECLI: [ECLI]

P≈ò√çKLAD DOBR√â ODPOVƒöDI:
"Podle rozhodnut√≠ Nejvy≈°≈°√≠ho soudu [^1] plat√≠, ≈æe [konkr√©tn√≠ z√°vƒõr z rozhodnut√≠]. Toto bylo potvrzeno i v p≈ô√≠padƒõ [^2], kde soud rozhodl, ≈æe [konkr√©tn√≠ z√°vƒõr]."

P≈ò√çKLAD ≈†PATN√â ODPOVƒöDI (HALUCINACE):
"Podle ¬ß 123 z√°kona XYZ..." (pokud tento paragraf nen√≠ v rozhodnut√≠ch)
"Obecnƒõ plat√≠, ≈æe..." (bez odkazu na konkr√©tn√≠ rozhodnut√≠)
"Soud by pravdƒõpodobnƒõ rozhodl..." (spekulace)

PAMATUJTE: Radƒõji ≈ôeknƒõte "nev√≠m" ne≈æ vym√Ω≈°lejte informace!"""

SONAR_PROMPT = """Jste pr√°vn√≠ expert se specialistem na ƒçesk√© pr√°vo. Odpov√≠dejte na ot√°zky u≈æivatele V√ùHRADNƒö na z√°kladƒõ poskytnut√Ωch rozhodnut√≠ ƒçesk√Ωch soud≈Ø.

Va≈°e odpovƒõƒè mus√≠ obsahovat:
1. P≈ô√≠mou odpovƒõƒè na polo≈æenou ot√°zku na z√°kladƒõ p≈ô√≠slu≈°n√Ωch rozhodnut√≠
2. Citace v≈°ech relevantn√≠ch, aktu√°ln√≠ch a konkr√©tn√≠ch z√°kon≈Ø, vyhl√°≈°ek, pr√°vn√≠ch p≈ôedpis≈Ø, pr√°vn√≠ch princip≈Ø, zr√°tka z√°kona, mus√≠ obsahovat:
   - Konkr√©tn√≠ paragraf a ƒç√≠slo z√°konu
   - Datum vyd√°n√≠
   - Datum vyd√°n√≠
   - ECLI reference
   - Relevantn√≠ pr√°vn√≠ p≈ôedpisy (¬ß citace)

Odpovƒõƒè mus√≠ b√Ωt:
- Strukturovan√° a logick√°
- Psan√° v ƒçe≈°tinƒõ
- Soust≈ôedƒõna v√Ωhradnƒõ na poskytnut√© informace
- Bez generalizac√≠ nebo informac√≠ mimo z√°kladnu rozhodnut√≠
- S p≈ôesn√Ωmi citacemi a odkazem
- Mus√≠ vych√°zet z kontextu, mus√≠ br√°t v potaz i pr√°vn√≠ principy, strukturu a hierarchii z√°kon≈Ø
- Pou≈æ√≠vejte pouze √∫daje z ofici√°ln√≠ch vl√°dn√≠ch nebo renomovan√Ωch pr√°vn√≠ch web≈Ø (nap≈ô. zakonyprolidi.cz, nsoud.cz, eur-lex.europa.eu)
- Vyh√Ωbejte se citac√≠m z n√°hodn√Ωch f√≥r, diskuzn√≠ch skupin nebo u≈æivatelsk√Ωch koment√°≈ô≈Ø

Pokud je ot√°zka nezodpovƒõditeln√° na z√°kladƒõ tƒõchto dat a tohoto postupu, v√Ωslovnƒõ to uveƒète."""


def get_openai_client() -> OpenAI:
    """Get configured OpenAI client for OpenRouter"""
    return OpenAI(
        api_key=settings.OPENROUTER_API_KEY,
        base_url=settings.OPENROUTER_BASE_URL,
    )


async def get_sonar_answer(question: str) -> tuple[str, list[str]]:
    """
    Get answer from Perplexity Sonar with citations
    Returns: (answer_text, citations_list)
    """
    try:
        client = get_openai_client()

        sonar_response = client.chat.completions.create(
            model="perplexity/sonar",
            messages=[
                {"role": "system", "content": SONAR_PROMPT},
                {"role": "user", "content": question},
            ],
            stream=False,
        )

        sonar_answer = sonar_response.choices[0].message.content or ""

        # Capture citations
        sonar_citations = getattr(sonar_response, "citations", [])
        if not sonar_citations:
            search_results = getattr(sonar_response, "search_results", [])
            sonar_citations = [
                result.get("url", "") for result in search_results if result.get("url")
            ]

        return sonar_answer, sonar_citations

    except Exception as e:
        print(f"Chyba pri ziskani Sonar odpovedi: {str(e)}")
        return "", []


async def get_sonar_answer_stream(question: str):
    """
    Get streaming answer from Perplexity Sonar with citations
    Yields: (chunk_text, final_answer, citations_list)
    
    Note: Perplexity's streaming API doesn't include citations in chunks.
    We need to make a separate non-streaming call to get citations.
    """
    try:
        client = get_openai_client()

        # Start streaming the answer
        stream = client.chat.completions.create(
            model="perplexity/sonar",
            messages=[
                {"role": "system", "content": SONAR_PROMPT},
                {"role": "user", "content": question},
            ],
            stream=True,
        )

        full_answer = ""
        citations = []

        # Stream the content
        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_answer += content
                yield content, None, None

        # After streaming completes, make a non-streaming call to get citations
        # This is necessary because Perplexity's streaming API doesn't include citations
        try:
            citation_response = client.chat.completions.create(
                model="perplexity/sonar",
                messages=[
                    {"role": "system", "content": SONAR_PROMPT},
                    {"role": "user", "content": question},
                ],
                stream=False,
            )
            
            # Extract citations from the response
            citations = getattr(citation_response, "citations", [])
            if not citations:
                # Fallback to search_results if citations not available
                search_results = getattr(citation_response, "search_results", [])
                citations = [
                    result.get("url", "") for result in search_results if result.get("url")
                ]
        except Exception as citation_error:
            print(f"Error fetching citations: {str(citation_error)}")
            citations = []

        # Final yield with complete answer and citations
        yield None, full_answer, citations

    except Exception as e:
        print(f"Chyba pri ziskani Sonar odpovedi: {str(e)}")
        yield None, "", []


async def answer_based_on_cases(
    question: str, cases: list[CaseResult], client: OpenAI
) -> str:
    """
    GPT-4o answers the question based on FULL case data with citations
    NO TRUNCATION - All context is passed to GPT
    """
    try:
        # Format cases with FULL context - NO TRUNCATION
        cases_context = format_cases_for_context(cases)
        
        print(f"\n{'='*80}")
        print(f"üì§ PASSING FULL CONTEXT TO GPT")
        print(f"{'='*80}")
        print(f"Number of cases: {len(cases)}")
        print(f"Context length: {len(cases_context)} characters")
        print(f"Context length: {len(cases_context.split())} words")
        print(f"Estimated tokens: ~{len(cases_context) // 4}")
        print(f"{'='*80}\n")

        response = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {
                    "role": "user",
                    "content": f"""OT√ÅZKA U≈ΩIVATELE:
{question}

POSKYTNUT√Å SOUDN√ç ROZHODNUT√ç (KOMPLETN√ç KONTEXT):
{cases_context}

√öKOL:
1. Analyzujte ka≈æd√© rozhodnut√≠ a zjistƒõte, zda obsahuje informace relevantn√≠ k ot√°zce
2. Pokud ANO: Vytvo≈ôte strukturovanou odpovƒõƒè s inline citacemi [^1], [^2] atd.
3. Pokud NE: Jasnƒõ ≈ôeknƒõte, ≈æe rozhodnut√≠ se net√Ωkaj√≠ t√©to ot√°zky
4. NIKDY nevym√Ω≈°lejte informace, kter√© nejsou v rozhodnut√≠ch
5. Citujte konkr√©tn√≠ ƒç√°sti rozhodnut√≠, ne obecn√© pr√°vn√≠ znalosti

D≈ÆLE≈ΩIT√â: M√°te k dispozici PLN√ù kontext v≈°ech rozhodnut√≠ bez zkr√°cen√≠.
Zaƒçnƒõte anal√Ωzou relevance rozhodnut√≠.""",
                },
            ],
            temperature=0.3,  # Sn√≠≈æen√° teplota pro men≈°√≠ halucinace
            max_tokens=4000,  # Increased for longer, more detailed responses
        )

        answer = (response.choices[0].message.content or "").strip()
        
        print(f"‚úÖ GPT response generated: {len(answer)} characters\n")
        
        return answer

    except Exception as e:
        print(f"‚ùå Chyba pri generovani odpovedi zalozene na pripadech: {str(e)}")
        import traceback
        traceback.print_exc()
        return ""


async def answer_based_on_cases_stream(
    question: str, cases: list[CaseResult], client: OpenAI
):
    """
    Stream GPT-4o answer based on FULL case data - NO TRUNCATION
    """
    try:
        print(f"\n{'='*80}")
        print(f"üì§ STREAMING FULL CONTEXT TO GPT")
        print(f"{'='*80}")
        print(f"Number of cases: {len(cases)}")
        
        # Format cases with FULL context - NO TRUNCATION
        cases_context = format_cases_for_context(cases)
        
        print(f"Context length: {len(cases_context)} characters")
        print(f"Context length: {len(cases_context.split())} words")
        print(f"Estimated tokens: ~{len(cases_context) // 4}")
        print(f"{'='*80}\n")

        print(f"ü§ñ Starting OpenAI streaming...")
        stream = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {
                    "role": "user",
                    "content": f"""OT√ÅZKA U≈ΩIVATELE:
{question}

POSKYTNUT√Å SOUDN√ç ROZHODNUT√ç (KOMPLETN√ç KONTEXT):
{cases_context}

√öKOL:
1. Analyzujte ka≈æd√© rozhodnut√≠ a zjistƒõte, zda obsahuje informace relevantn√≠ k ot√°zce
2. Pokud ANO: Vytvo≈ôte strukturovanou odpovƒõƒè s inline citacemi [^1], [^2] atd.
3. Pokud NE: Jasnƒõ ≈ôeknƒõte, ≈æe rozhodnut√≠ se net√Ωkaj√≠ t√©to ot√°zky
4. NIKDY nevym√Ω≈°lejte informace, kter√© nejsou v rozhodnut√≠ch
5. Citujte konkr√©tn√≠ ƒç√°sti rozhodnut√≠, ne obecn√© pr√°vn√≠ znalosti

D≈ÆLE≈ΩIT√â: M√°te k dispozici PLN√ù kontext v≈°ech rozhodnut√≠ bez zkr√°cen√≠.
Zaƒçnƒõte anal√Ωzou relevance rozhodnut√≠.""",
                },
            ],
            temperature=0.3,  # Sn√≠≈æen√° teplota pro men≈°√≠ halucinace
            max_tokens=4000,  # Increased for longer, more detailed responses
            stream=True,
        )

        chunk_count = 0
        for chunk in stream:
            if chunk.choices[0].delta.content:
                chunk_count += 1
                content = chunk.choices[0].delta.content
                yield content
        
        print(f"‚úÖ Yielded {chunk_count} chunks from OpenAI")
        
        if chunk_count == 0:
            print("‚ö†Ô∏è WARNING: OpenAI returned 0 chunks!")

    except Exception as e:
        print(f"‚ùå Chyba pri streamovani odpovedi: {str(e)}")
        import traceback
        traceback.print_exc()



async def generate_combined_summary_stream(
    question: str,
    web_answer: str,
    case_answer: str,
    client: OpenAI
):
    """
    Generate a concise summary combining web and case search results
    """
    try:
        summary_prompt = """Jste pr√°vn√≠ expert. M√°te k dispozici dvƒõ odpovƒõdi na stejnou ot√°zku:
1. Odpovƒõƒè z webov√©ho vyhled√°v√°n√≠ (aktu√°ln√≠ pr√°vn√≠ informace)
2. Odpovƒõƒè zalo≈æen√° na soudn√≠ch rozhodnut√≠ch (judikatura)

Vytvo≈ôte KR√ÅTK√â shrnut√≠ (2-3 vƒõty), kter√©:
- Syntetizuje obƒõ odpovƒõdi
- Zd≈Ørazn√≠ kl√≠ƒçov√© body
- Uk√°≈æe, jak se webov√© informace a judikatura dopl≈àuj√≠
- Buƒète struƒçn√Ω a jasn√Ω

NEOPISUJTE cel√© odpovƒõdi, pouze shr≈àte hlavn√≠ z√°vƒõry."""

        stream = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[
                {"role": "system", "content": summary_prompt},
                {
                    "role": "user",
                    "content": f"""OT√ÅZKA:
{question}

WEBOV√Å ODPOVƒöƒé:
{web_answer[:1000]}

ODPOVƒöƒé ZE SOUDN√çCH ROZHODNUT√ç:
{case_answer[:1000]}

Vytvo≈ôte kr√°tk√© shrnut√≠ (2-3 vƒõty):"""
                }
            ],
            temperature=0.3,
            max_tokens=300,
            stream=True,
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    except Exception as e:
        print(f"Error generating summary: {str(e)}")
        yield ""
